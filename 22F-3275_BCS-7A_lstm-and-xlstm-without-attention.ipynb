{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13124318,"sourceType":"datasetVersion","datasetId":8313870}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Urdu to Roman Urdu Neural Machine Translation \n\n\n\nThis notebook contains:\n1. Data preprocessing and tokenization\n2. BiLSTM Encoder-Decoder model without attention\n3. Fixed training loop with proper evaluation\n4.  BLEU score calculation\n5. Perplexity and CER evaluation metrics\n6.  Training function with multiple metrics\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Imports and Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport pickle\nimport random\nimport math\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\nimport unicodedata\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Core libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\n\n# !pip install sentencepiece\n\nimport sentencepiece as spm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:17.758924Z","iopub.execute_input":"2025-09-25T09:00:17.759533Z","iopub.status.idle":"2025-09-25T09:00:17.764500Z","shell.execute_reply.started":"2025-09-25T09:00:17.759508Z","shell.execute_reply":"2025-09-25T09:00:17.763920Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Set random seeds\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:18.131337Z","iopub.execute_input":"2025-09-25T09:00:18.131564Z","iopub.status.idle":"2025-09-25T09:00:18.137488Z","shell.execute_reply.started":"2025-09-25T09:00:18.131546Z","shell.execute_reply":"2025-09-25T09:00:18.136800Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## 2. Data Preprocessing Class","metadata":{}},{"cell_type":"code","source":"\nclass UrduRomanDataProcessor:\n\n    \n    def __init__(self, dataset_path: str):\n        self.dataset_path = Path(dataset_path)\n        self.urdu_texts = []\n        self.roman_texts = []\n    \n    def load_data(self):\n\n        print(\"Loading data from urdu_ghazals_rekhta dataset...\")\n        \n        if not self.dataset_path.exists():\n            raise FileNotFoundError(f\"Dataset path {self.dataset_path} not found\")\n        \n        # Dataset structure: poets -> [ur, en, hi] -> files (no extensions)\n        for poet_dir in self.dataset_path.iterdir():\n            if not poet_dir.is_dir() or poet_dir.name.startswith('.'):\n                continue\n                \n            urdu_dir = poet_dir / 'ur'\n            english_dir = poet_dir / 'en'  # This contains Roman Urdu transliteration\n            \n            if not (urdu_dir.exists() and english_dir.exists()):\n                continue\n            \n            # Get all Urdu files (no extension filter needed)\n            urdu_files = [f for f in urdu_dir.iterdir() if f.is_file() and not f.name.startswith('.')]\n            \n            for urdu_file in urdu_files:\n                english_file = english_dir / urdu_file.name\n                \n                if english_file.exists() and english_file.is_file():\n                    try:\n                        # Read Urdu text\n                        with open(urdu_file, 'r', encoding='utf-8') as f:\n                            urdu_content = f.read().strip()\n                        \n                        # Read Roman Urdu text\n                        with open(english_file, 'r', encoding='utf-8') as f:\n                            roman_content = f.read().strip()\n                        \n                        # Split by lines to get verse pairs\n                        urdu_lines = [line.strip() for line in urdu_content.split('\\n') if line.strip()]\n                        roman_lines = [line.strip() for line in roman_content.split('\\n') if line.strip()]\n                        \n                        # Pair up lines (verses)\n                        for urdu_line, roman_line in zip(urdu_lines, roman_lines):\n                            if urdu_line and roman_line:\n                                self.urdu_texts.append(urdu_line)\n                                self.roman_texts.append(roman_line)\n                                \n                    except Exception as e:\n                        print(f\"Error reading {urdu_file.name}: {e}\")\n                        continue\n        \n        print(f\"Loaded {len(self.urdu_texts)} text pairs\")\n        \n        if len(self.urdu_texts) == 0:\n            raise ValueError(\"No data loaded. Check dataset structure and paths.\")\n    \n    def clean_text(self, text: str, is_urdu: bool = True) -> str:\n        \"\"\"Clean and normalize text\"\"\"\n        # Unicode normalization\n        text = unicodedata.normalize('NFKC', text)\n        \n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        if is_urdu:\n            # Keep Urdu characters and basic punctuation\n            text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\\"\\']+', '', text)\n        else:\n            # Convert to lowercase and keep Roman characters\n            text = text.lower()\n            text = re.sub(r'[^a-z0-9\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\\"\\']+', '', text)\n        \n        return text.strip()\n    \n    def preprocess_data(self, min_words=3, max_words=50):\n        \"\"\"Clean and filter the data\"\"\"\n        print(\"Preprocessing and filtering data...\")\n        \n        cleaned_urdu = []\n        cleaned_roman = []\n        \n        for urdu, roman in zip(self.urdu_texts, self.roman_texts):\n            # Clean texts\n            clean_urdu = self.clean_text(urdu, is_urdu=True)\n            clean_roman = self.clean_text(roman, is_urdu=False)\n            \n            # Filter by length\n            urdu_words = len(clean_urdu.split())\n            roman_words = len(clean_roman.split())\n            \n            if (min_words <= urdu_words <= max_words and \n                min_words <= roman_words <= max_words and \n                clean_urdu and clean_roman):\n                cleaned_urdu.append(clean_urdu)\n                cleaned_roman.append(clean_roman)\n        \n        self.urdu_texts = cleaned_urdu\n        self.roman_texts = cleaned_roman\n        \n        print(f\"After preprocessing: {len(self.urdu_texts)} pairs\")\n        \n        if len(self.urdu_texts) < 100:\n            print(\"Warning: Very few text pairs available. Consider relaxing filtering criteria.\")\n    \n    def split_data(self, test_size=0.25, val_size=0.25, random_state=42):\n        \"\"\"Split data into train/val/test sets (50/25/25 as required)\"\"\"\n        # First split: separate test set (25%)\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            self.urdu_texts, self.roman_texts, \n            test_size=test_size, random_state=random_state\n        )\n        \n        # Second split: divide remaining into train/val\n        val_adjusted = val_size / (1 - test_size)  # 0.25 / 0.75 = 0.333\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_temp, y_temp, \n            test_size=val_adjusted, random_state=random_state\n        )\n        \n        print(f\"Data split - Train: {len(X_train)} ({len(X_train)/len(self.urdu_texts)*100:.1f}%), \"\n              f\"Val: {len(X_val)} ({len(X_val)/len(self.urdu_texts)*100:.1f}%), \"\n              f\"Test: {len(X_test)} ({len(X_test)/len(self.urdu_texts)*100:.1f}%)\")\n        \n        return {\n            'train': {'urdu': X_train, 'roman': y_train},\n            'val': {'urdu': X_val, 'roman': y_val},\n            'test': {'urdu': X_test, 'roman': y_test}\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:18.624952Z","iopub.execute_input":"2025-09-25T09:00:18.625242Z","iopub.status.idle":"2025-09-25T09:00:18.639388Z","shell.execute_reply.started":"2025-09-25T09:00:18.625220Z","shell.execute_reply":"2025-09-25T09:00:18.638687Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"\ndef create_tokenizers(train_urdu, train_roman, vocab_size=8000):\n  \n    print(f\"Training SentencePiece tokenizers with vocab_size={vocab_size}...\")\n    \n    # Create tokenizers directory\n    os.makedirs('tokenizers', exist_ok=True)\n    \n    # Save training data\n    with open('tokenizers/urdu_train.txt', 'w', encoding='utf-8') as f:\n        for text in train_urdu:\n            f.write(text + '\\n')\n    \n    with open('tokenizers/roman_train.txt', 'w', encoding='utf-8') as f:\n        for text in train_roman:\n            f.write(text + '\\n')\n    \n    # Estimate reasonable vocab size based on data\n    def estimate_vocab_size(texts, target_size):\n        all_text = ' '.join(texts)\n        unique_chars = len(set(all_text))\n        unique_words = len(set(all_text.split()))\n        # Use target size but cap at reasonable limits\n        return min(target_size, max(unique_chars * 10, 1000), unique_words)\n    \n    urdu_vocab_size = estimate_vocab_size(train_urdu, vocab_size)\n    roman_vocab_size = estimate_vocab_size(train_roman, vocab_size)\n    \n    print(f\"Adjusted vocab sizes - Urdu: {urdu_vocab_size}, Roman: {roman_vocab_size}\")\n    \n    # Train Urdu tokenizer\n    spm.SentencePieceTrainer.train(\n        input='tokenizers/urdu_train.txt',\n        model_prefix='tokenizers/urdu_tokenizer',\n        vocab_size=urdu_vocab_size,\n        model_type='unigram',\n        character_coverage=1.0,\n        pad_id=0, unk_id=1, bos_id=2, eos_id=3\n    )\n    \n    # Train Roman tokenizer\n    spm.SentencePieceTrainer.train(\n        input='tokenizers/roman_train.txt',\n        model_prefix='tokenizers/roman_tokenizer',\n        vocab_size=roman_vocab_size,\n        model_type='unigram',\n        character_coverage=1.0,\n        pad_id=0, unk_id=1, bos_id=2, eos_id=3\n    )\n    \n    # Load trained models\n    urdu_tokenizer = spm.SentencePieceProcessor(model_file='tokenizers/urdu_tokenizer.model')\n    roman_tokenizer = spm.SentencePieceProcessor(model_file='tokenizers/roman_tokenizer.model')\n    \n    print(f\"Final vocab sizes - Urdu: {urdu_tokenizer.get_piece_size()}, \"\n          f\"Roman: {roman_tokenizer.get_piece_size()}\")\n    \n    return urdu_tokenizer, roman_tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:18.783890Z","iopub.execute_input":"2025-09-25T09:00:18.784242Z","iopub.status.idle":"2025-09-25T09:00:18.791469Z","shell.execute_reply.started":"2025-09-25T09:00:18.784220Z","shell.execute_reply":"2025-09-25T09:00:18.790788Z"}},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":"## 3. PyTorch Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    \"\"\"PyTorch Dataset for translation pairs\"\"\"\n    \n    def __init__(self, urdu_texts, roman_texts, urdu_tokenizer, roman_tokenizer, max_length=50):\n        self.urdu_texts = urdu_texts\n        self.roman_texts = roman_texts\n        self.urdu_tokenizer = urdu_tokenizer\n        self.roman_tokenizer = roman_tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.urdu_texts)\n    \n    def __getitem__(self, idx):\n        urdu_text = self.urdu_texts[idx]\n        roman_text = self.roman_texts[idx]\n        \n        # Tokenize\n        urdu_tokens = self.urdu_tokenizer.encode(urdu_text, out_type=int)\n        roman_tokens = self.roman_tokenizer.encode(roman_text, out_type=int)\n        \n        # Truncate if too long\n        urdu_tokens = urdu_tokens[:self.max_length]\n        roman_tokens = roman_tokens[:self.max_length]\n        \n        return {\n            'urdu': torch.tensor(urdu_tokens, dtype=torch.long),\n            'roman': torch.tensor(roman_tokens, dtype=torch.long),\n            'urdu_text': urdu_text,\n            'roman_text': roman_text\n        }\n\ndef collate_fn(batch):\n    \"\"\"Collate function for DataLoader with padding\"\"\"\n    urdu_seqs = [item['urdu'] for item in batch]\n    roman_seqs = [item['roman'] for item in batch]\n    \n    # Pad sequences\n    urdu_padded = nn.utils.rnn.pad_sequence(urdu_seqs, batch_first=True, padding_value=0)\n    roman_padded = nn.utils.rnn.pad_sequence(roman_seqs, batch_first=True, padding_value=0)\n    \n    return {\n        'urdu': urdu_padded,\n        'roman': roman_padded,\n        'urdu_texts': [item['urdu_text'] for item in batch],\n        'roman_texts': [item['roman_text'] for item in batch]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:19.170325Z","iopub.execute_input":"2025-09-25T09:00:19.170551Z","iopub.status.idle":"2025-09-25T09:00:19.177498Z","shell.execute_reply.started":"2025-09-25T09:00:19.170534Z","shell.execute_reply":"2025-09-25T09:00:19.176771Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"## 4. BiLSTM Encoder (2 layers)","metadata":{}},{"cell_type":"code","source":"\n\nclass BiLSTMEncoder(nn.Module):\n    \"\"\"BiLSTM Encoder (2 layers as required)\"\"\"\n    \n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.1):\n        super(BiLSTMEncoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim, hidden_dim, num_layers, \n            batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, lengths=None):\n        # x: (batch_size, seq_len)\n        batch_size = x.size(0)\n        \n        # Embedding\n        embedded = self.dropout(self.embedding(x))\n        \n        # Pack padded sequence for efficiency if lengths provided\n        if lengths is not None:\n            packed = nn.utils.rnn.pack_padded_sequence(\n                embedded, lengths, batch_first=True, enforce_sorted=False\n            )\n            outputs, (hidden, cell) = self.lstm(packed)\n            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        else:\n            outputs, (hidden, cell) = self.lstm(embedded)\n        \n        return outputs, hidden, cell\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:19.789815Z","iopub.execute_input":"2025-09-25T09:00:19.790486Z","iopub.status.idle":"2025-09-25T09:00:19.796294Z","shell.execute_reply.started":"2025-09-25T09:00:19.790465Z","shell.execute_reply":"2025-09-25T09:00:19.795656Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"## 5. Tokenization","metadata":{}},{"cell_type":"code","source":"\n\ndef create_tokenizers(urdu_texts, roman_texts, vocab_size=8000):\n    \"\"\"Create SentencePiece tokenizers for Urdu and Roman text\"\"\"\n    import tempfile\n    import os\n    \n    # Create temporary files for training data\n    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n        urdu_temp_file = f.name\n        for text in urdu_texts:\n            f.write(text + '\\n')\n    \n    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n        roman_temp_file = f.name\n        for text in roman_texts:\n            f.write(text + '\\n')\n    \n    try:\n        # Train Urdu tokenizer\n        spm.SentencePieceTrainer.train(\n            input=urdu_temp_file,\n            model_prefix='urdu_tokenizer',\n            vocab_size=vocab_size,\n            character_coverage=1.0,\n            model_type='bpe',\n            pad_id=0,\n            unk_id=1,\n            bos_id=2,\n            eos_id=3\n        )\n        \n        # Train Roman tokenizer\n        spm.SentencePieceTrainer.train(\n            input=roman_temp_file,\n            model_prefix='roman_tokenizer',\n            vocab_size=vocab_size,\n            character_coverage=1.0,\n            model_type='bpe',\n            pad_id=0,\n            unk_id=1,\n            bos_id=2,\n            eos_id=3\n        )\n        \n        # Load tokenizers\n        urdu_tokenizer = spm.SentencePieceProcessor()\n        urdu_tokenizer.load('urdu_tokenizer.model')\n        \n        roman_tokenizer = spm.SentencePieceProcessor()\n        roman_tokenizer.load('roman_tokenizer.model')\n        \n        print(f\"Urdu tokenizer vocabulary size: {urdu_tokenizer.get_piece_size()}\")\n        print(f\"Roman tokenizer vocabulary size: {roman_tokenizer.get_piece_size()}\")\n        \n        return urdu_tokenizer, roman_tokenizer\n        \n    finally:\n        # Clean up temporary files\n        try:\n            os.unlink(urdu_temp_file)\n            os.unlink(roman_temp_file)\n        except:\n            pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:20.214633Z","iopub.execute_input":"2025-09-25T09:00:20.214855Z","iopub.status.idle":"2025-09-25T09:00:20.221676Z","shell.execute_reply.started":"2025-09-25T09:00:20.214838Z","shell.execute_reply":"2025-09-25T09:00:20.221012Z"}},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"## 6. LSTM Decoder with Attention (4 layers)","metadata":{}},{"cell_type":"code","source":"\nclass LSTMDecoder(nn.Module):\n    \"\"\"LSTM Decoder without attention (4 layers as required)\"\"\"\n    \n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=4, dropout=0.1):\n        super(LSTMDecoder, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        \n        # Fixed encoder context: project mean-pooled encoder outputs to embedding dim\n        self.context_proj = nn.Linear(hidden_dim * 2, embed_dim)\n        \n        # Decoder operates on [embedding ; fixed_context]\n        self.lstm = nn.LSTM(\n            embed_dim * 2, hidden_dim, num_layers,\n            batch_first=True, dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Weight tying: map hidden_dim -> embed_dim, then tie to embedding weight for logits\n        self.h2e = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, hidden_state, encoder_outputs, encoder_mask=None):\n        # x: (batch_size, 1) - single time step\n        if x.dim() == 1:\n            x = x.unsqueeze(1)\n        \n        embedded = self.dropout(self.embedding(x))\n        \n        # Compute fixed context (masked mean over encoder sequence)\n        if encoder_mask is not None:\n            lengths = encoder_mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n            context = (encoder_outputs * encoder_mask.unsqueeze(-1)).sum(dim=1) / lengths\n        else:\n            context = encoder_outputs.mean(dim=1)\n        context = self.context_proj(context).unsqueeze(1)  # (batch, 1, embed_dim)\n        \n        # Concatenate embedding with fixed context\n        dec_input = torch.cat([embedded, context], dim=-1)\n        \n        output, hidden_state = self.lstm(dec_input, hidden_state)\n        \n        # Project to embedding space and tie with embedding weights to produce logits\n        logits = F.linear(self.h2e(output), self.embedding.weight)\n        \n        return logits, hidden_state, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:25.159529Z","iopub.execute_input":"2025-09-25T09:00:25.159766Z","iopub.status.idle":"2025-09-25T09:00:25.167058Z","shell.execute_reply.started":"2025-09-25T09:00:25.159749Z","shell.execute_reply":"2025-09-25T09:00:25.166255Z"}},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"## 7. Complete Seq2Seq Model","metadata":{}},{"cell_type":"code","source":"\nclass Seq2SeqModel(nn.Module):\n    \"\"\"Seq2Seq model with BiLSTM encoder and LSTM decoder\"\"\"\n    \n    def __init__(self, urdu_vocab_size, roman_vocab_size, embed_dim=128, hidden_dim=256,\n                 encoder_layers=2, decoder_layers=4, dropout=0.1, decoder_word_dropout=0.0):\n        super(Seq2SeqModel, self).__init__()\n        \n        self.encoder = BiLSTMEncoder(urdu_vocab_size, embed_dim, hidden_dim, encoder_layers, dropout)\n        self.decoder = LSTMDecoder(roman_vocab_size, embed_dim, hidden_dim, decoder_layers, dropout)\n        \n        self.hidden_dim = hidden_dim\n        self.encoder_layers = encoder_layers\n        self.decoder_layers = decoder_layers\n        \n        self.bridge_h = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.bridge_c = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        # Decoder word dropout probability used during teacher forcing\n        self.decoder_word_dropout = decoder_word_dropout\n\n    def forward(self, urdu_seq, roman_seq=None, teacher_forcing_ratio=0.5):\n        batch_size = urdu_seq.size(0)\n        device = urdu_seq.device\n        \n        encoder_mask = (urdu_seq != 0).float()\n        encoder_lengths = encoder_mask.sum(dim=1).cpu()\n        \n        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(urdu_seq, encoder_lengths)\n        \n        encoder_hidden = encoder_hidden.view(self.encoder_layers, 2, batch_size, self.hidden_dim)\n        encoder_cell = encoder_cell.view(self.encoder_layers, 2, batch_size, self.hidden_dim)\n        \n        last_hidden = torch.cat([encoder_hidden[-1, 0], encoder_hidden[-1, 1]], dim=1)\n        last_cell = torch.cat([encoder_cell[-1, 0], encoder_cell[-1, 1]], dim=1)\n        \n        # Use tanh bridge for stability\n        decoder_hidden = torch.tanh(self.bridge_h(last_hidden)).unsqueeze(0).repeat(self.decoder_layers, 1, 1)\n        decoder_cell = torch.tanh(self.bridge_c(last_cell)).unsqueeze(0).repeat(self.decoder_layers, 1, 1)\n        \n        if roman_seq is not None:\n            max_length = roman_seq.size(1)\n            outputs = []\n            input_token = roman_seq[:, 0:1]\n            hidden_state = (decoder_hidden, decoder_cell)\n            \n            for t in range(max_length - 1):\n                output, hidden_state, _ = self.decoder(\n                    input_token, hidden_state, encoder_outputs, encoder_mask\n                )\n                outputs.append(output)\n                \n                # Decide next input\n                if random.random() < teacher_forcing_ratio:\n                    next_token = roman_seq[:, t+1:t+2]\n                    # Apply decoder word dropout on teacher-forced inputs\n                    if self.training and self.decoder_word_dropout > 0.0:\n                        # Replace with UNK (id=1) with probability p\n                        dropout_mask = (torch.rand_like(next_token.float()) < self.decoder_word_dropout)\n                        next_token = next_token.masked_fill(dropout_mask, 1)\n                    input_token = next_token\n                else:\n                    input_token = output.argmax(dim=-1)\n            \n            return torch.cat(outputs, dim=1)\n        else:\n            # Dynamic decode length based on input length\n            dyn_max_len = int(encoder_lengths.max().item() * 1.5) + 10\n            dyn_max_len = max(dyn_max_len, 20)\n            return self.beam_search_decode(\n                encoder_outputs, encoder_mask, decoder_hidden, decoder_cell,\n                beam_size=5, max_length=dyn_max_len\n            )\n    \n    def beam_search_decode(self, encoder_outputs, encoder_mask, decoder_hidden, \n                          decoder_cell, beam_size=5, max_length=50):\n        \"\"\"Beam search decoding to avoid repetition with length penalty\"\"\"\n        batch_size = encoder_outputs.size(0)\n        device = encoder_outputs.device\n        \n        if batch_size > 1:\n            return self.greedy_decode(encoder_outputs, encoder_mask, decoder_hidden, \n                                    decoder_cell, max_length)\n        \n        beams = [([], 0.0, (decoder_hidden, decoder_cell))]\n        completed = []\n        start_token = torch.tensor([[2]], dtype=torch.long, device=device)\n        \n        alpha = 0.6  # length penalty factor\n        unk_id = 1   # UNK token id\n        \n        for step in range(max_length):\n            candidates = []\n            \n            for seq, score, hidden_state in beams:\n                if len(seq) > 0 and seq[-1] == 3:\n                    completed.append((seq, score))\n                    continue\n                \n                input_token = start_token if len(seq) == 0 else torch.tensor([[seq[-1]]], dtype=torch.long, device=device)\n                \n                output, new_hidden, _ = self.decoder(\n                    input_token, hidden_state, encoder_outputs, encoder_mask\n                )\n                \n                log_probs = F.log_softmax(output.squeeze(1), dim=-1)\n                # Penalize UNK\n                if log_probs.size(-1) > unk_id:\n                    log_probs[0, unk_id] -= 2.0\n                \n                top_k_scores, top_k_tokens = log_probs.topk(beam_size)\n                \n                for k in range(beam_size):\n                    token = top_k_tokens[0, k].item()\n                    token_score = top_k_scores[0, k].item()\n                    \n                    if len(seq) > 0 and token == seq[-1]:\n                        token_score -= 2.0\n                    if len(seq) > 3:\n                        recent = seq[-3:]\n                        if recent.count(token) > 1:\n                            token_score -= 3.0\n                    \n                    new_seq = seq + [token]\n                    new_score = score + token_score\n                    candidates.append((new_seq, new_score, new_hidden))\n            \n            candidates.sort(key=lambda x: x[1], reverse=True)\n            beams = candidates[:beam_size]\n            \n            if len(beams) == 0:\n                break\n        \n        completed.extend(beams)\n        \n        if completed:\n            def lp(s):\n                L = max(1, len(s))\n                return L ** alpha\n            best_seq = max(completed, key=lambda x: x[1] / lp(x[0]))[0]\n        else:\n            best_seq = beams[0][0] if beams else []\n        \n        if best_seq:\n            vocab_size = self.decoder.vocab_size\n            output_probs = torch.zeros(1, len(best_seq), vocab_size, device=device)\n            for i, token in enumerate(best_seq):\n                output_probs[0, i, token] = 1.0\n        else:\n            # Emit a single EOS step to avoid zero-length outputs\n            vocab_size = self.decoder.vocab_size\n            output_probs = torch.zeros(1, 1, vocab_size, device=device)\n            output_probs[0, 0, 3] = 1.0  # EOS\n        \n        return output_probs\n    \n    def greedy_decode(self, encoder_outputs, encoder_mask, decoder_hidden, \n                     decoder_cell, max_length=50):\n        \"\"\"Greedy decoding with repetition penalty\"\"\"\n        batch_size = encoder_outputs.size(0)\n        device = encoder_outputs.device\n        vocab_size = self.decoder.vocab_size\n        \n        outputs = []\n        input_token = torch.full((batch_size, 1), 2, dtype=torch.long, device=device)  # SOS\n        hidden_state = (decoder_hidden, decoder_cell)\n        \n        # Track recent tokens for repetition penalty\n        recent_tokens = []\n        unk_id = 1\n        \n        for step in range(max_length):\n            output, hidden_state, _ = self.decoder(\n                input_token, hidden_state, encoder_outputs, encoder_mask\n            )\n            \n            # Penalize UNK\n            if output.size(-1) > unk_id:\n                output[:, :, unk_id] -= 2.0\n            \n            # Apply repetition penalty\n            if len(recent_tokens) > 0:\n                for recent_token in recent_tokens[-3:]:  # Penalize last 3 tokens\n                    if recent_token < output.size(-1):\n                        output[0, 0, recent_token] -= 5.0\n            \n            outputs.append(output)\n            \n            # Get next token\n            input_token = output.argmax(dim=-1)\n            token_id = input_token.item() if batch_size == 1 else input_token[0].item()\n            \n            recent_tokens.append(token_id)\n            \n            # Stop if EOS token\n            if token_id == 3:\n                break\n        \n        return torch.cat(outputs, dim=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:25.523614Z","iopub.execute_input":"2025-09-25T09:00:25.523881Z","iopub.status.idle":"2025-09-25T09:00:25.545187Z","shell.execute_reply.started":"2025-09-25T09:00:25.523859Z","shell.execute_reply":"2025-09-25T09:00:25.544148Z"}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"## 8. Evaluation","metadata":{}},{"cell_type":"code","source":"\n\ndef calculate_perplexity(model, data_loader, criterion, device):\n    \"\"\"Calculate perplexity\"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            urdu_seq = batch['urdu'].to(device)\n            roman_seq = batch['roman'].to(device)\n            \n            decoder_target = roman_seq[:, 1:]\n            # Use full teacher forcing to compute true NLL\n            outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio=1.0)\n            \n            loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n            non_pad_tokens = (decoder_target != 0).sum().item()\n            \n            total_loss += loss.item() * non_pad_tokens\n            total_tokens += non_pad_tokens\n    \n    if total_tokens == 0:\n        return float('inf')\n    \n    avg_loss = total_loss / total_tokens\n    return math.exp(avg_loss)\n\ndef calculate_cer(predictions, targets, tokenizer):\n    \"\"\"Calculate Character Error Rate\"\"\"\n    def edit_distance_cer(s1, s2):\n        if len(s1) < len(s2):\n            return edit_distance_cer(s2, s1)\n        \n        if len(s2) == 0:\n            return len(s1)\n        \n        previous_row = range(len(s2) + 1)\n        for i, c1 in enumerate(s1):\n            current_row = [i + 1]\n            for j, c2 in enumerate(s2):\n                insertions = previous_row[j + 1] + 1\n                deletions = current_row[j] + 1\n                substitutions = previous_row[j] + (c1 != c2)\n                current_row.append(min(insertions, deletions, substitutions))\n            previous_row = current_row\n        \n        return previous_row[-1]\n    \n    total_chars = 0\n    total_errors = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Convert to lists and remove special tokens\n        if hasattr(pred, 'tolist'):\n            pred_tokens = pred.tolist()\n        else:\n            pred_tokens = list(pred)\n        \n        if hasattr(target, 'tolist'):\n            target_tokens = target.tolist()\n        else:\n            target_tokens = list(target)\n        \n        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n        \n        if len(pred_clean) > 0 and len(target_clean) > 0:\n            pred_text = tokenizer.decode(pred_clean)\n            target_text = tokenizer.decode(target_clean)\n            \n            errors = edit_distance_cer(pred_text, target_text)\n            total_errors += errors\n            total_chars += len(target_text)\n    \n    return total_errors / total_chars if total_chars > 0 else 1.0\n\ndef edit_distance(s1, s2):\n    \n    \"\"\"Calculate edit distance (Levenshtein distance) between two strings\"\"\"\n    if len(s1) < len(s2):\n        return edit_distance(s2, s1)\n    \n    if len(s2) == 0:\n        return len(s1)\n    \n    previous_row = range(len(s2) + 1)\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1\n            deletions = current_row[j] + 1\n            substitutions = previous_row[j] + (c1 != c2)\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n    \n    return previous_row[-1]\n\ndef calculate_bleu_score(predictions, targets, tokenizer):\n    \"\"\"Calculate BLEU score properly\"\"\"\n    from collections import Counter\n    \n    def get_ngrams(tokens, n):\n        if len(tokens) < n:\n            return []\n        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n    \n    def calculate_bleu(pred_tokens, target_tokens, max_n=4):\n        if len(pred_tokens) == 0 or len(target_tokens) == 0:\n            return 0.0\n        \n        precisions = []\n        for n in range(1, min(max_n + 1, len(pred_tokens) + 1)):\n            pred_ngrams = Counter(get_ngrams(pred_tokens, n))\n            target_ngrams = Counter(get_ngrams(target_tokens, n))\n            \n            if len(pred_ngrams) == 0:\n                precisions.append(0.0)\n                continue\n            \n            matches = sum((pred_ngrams & target_ngrams).values())\n            total = sum(pred_ngrams.values())\n            precision = matches / total if total > 0 else 0.0\n            precisions.append(precision)\n        \n        # Brevity penalty\n        if len(pred_tokens) == 0:\n            bp = 0.0\n        elif len(pred_tokens) < len(target_tokens):\n            bp = math.exp(1 - len(target_tokens) / len(pred_tokens))\n        else:\n            bp = 1.0\n        \n        # Geometric mean of precisions\n        if precisions and all(p > 0 for p in precisions):\n            geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n            score = bp * geo_mean\n        else:\n            score = 0.0\n        \n        return score\n    \n    total_score = 0.0\n    count = 0\n    total_errors = 0\n    total_chars = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Handle tensors\n        if hasattr(pred, 'cpu'):\n            pred = pred.cpu()\n        if hasattr(target, 'cpu'):\n            target = target.cpu()\n        \n        # Convert to lists\n        if hasattr(pred, 'tolist'):\n            pred_tokens = pred.tolist()\n        else:\n            pred_tokens = list(pred)\n        \n        if hasattr(target, 'tolist'):\n            target_tokens = target.tolist()\n        else:\n            target_tokens = list(target)\n        \n        # Remove special tokens\n        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n        \n        if len(pred_clean) > 0 and len(target_clean) > 0:\n            # Calculate BLEU score\n            bleu = calculate_bleu(pred_clean, target_clean)\n            total_score += bleu\n            count += 1\n            \n            # Calculate CER (Character Error Rate)\n            pred_text = tokenizer.decode(pred_clean)\n            target_text = tokenizer.decode(target_clean)\n            \n            if len(pred_text) > 0 and len(target_text) > 0:\n                errors = edit_distance(pred_text, target_text)\n                total_errors += errors\n                total_chars += len(target_text)\n    \n    # Return BLEU score (not CER)\n    return total_score / count if count > 0 else 0.0\n\ndef calculate_accuracy(predictions, targets, tokenizer):\n    \n    \"\"\"Calculate token-level and sequence-level accuracy\"\"\"\n    total_tokens = 0\n    correct_tokens = 0\n    total_sequences = 0\n    correct_sequences = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Convert to lists and remove special tokens\n        if hasattr(pred, 'tolist'):\n            pred_tokens = pred.tolist()\n        else:\n            pred_tokens = list(pred)\n        \n        if hasattr(target, 'tolist'):\n            target_tokens = target.tolist()\n        else:\n            target_tokens = list(target)\n        \n        # Remove special tokens (padding, start, end, unknown)\n        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n        \n        if len(pred_clean) > 0 and len(target_clean) > 0:\n            # Token-level accuracy\n            min_len = min(len(pred_clean), len(target_clean))\n            max_len = max(len(pred_clean), len(target_clean))\n            \n            # Count matching tokens up to the minimum length\n            matches = sum(1 for i in range(min_len) if pred_clean[i] == target_clean[i])\n            correct_tokens += matches\n            total_tokens += max_len  # Use max length to penalize length differences\n            \n            # Sequence-level accuracy (exact match)\n            if pred_clean == target_clean:\n                correct_sequences += 1\n            total_sequences += 1\n    \n    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n    sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0.0\n    \n    return {\n        'token_accuracy': token_accuracy,\n        'sequence_accuracy': sequence_accuracy\n    }\n\ndef evaluate_model(model, data_loader, criterion, roman_tokenizer):\n    \n    \"\"\"Evaluate model\"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            urdu_seq = batch['urdu'].to(device)\n            roman_seq = batch['roman'].to(device)\n            \n            decoder_target = roman_seq[:, 1:]\n            \n            # Loss calculation with teacher forcing\n            outputs_tf = model(urdu_seq, roman_seq, teacher_forcing_ratio=1.0)\n            loss = criterion(outputs_tf.reshape(-1, outputs_tf.size(-1)), decoder_target.reshape(-1))\n            non_pad_tokens = (decoder_target != 0).sum().item()\n            \n            total_loss += loss.item() * non_pad_tokens\n            total_tokens += non_pad_tokens\n            \n            # Inference predictions using decoding (beam/greedy)\n            infer_outputs = model(urdu_seq)\n            pred_tokens = infer_outputs.argmax(dim=-1)\n            for i in range(pred_tokens.size(0)):\n                predictions.append(pred_tokens[i].cpu())\n                targets.append(decoder_target[i].cpu())\n    \n    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n    bleu = calculate_bleu_score(predictions, targets, roman_tokenizer)\n    perplexity = math.exp(avg_loss) if avg_loss != float('inf') else float('inf')\n    cer = calculate_cer(predictions, targets, roman_tokenizer)\n    accuracy_metrics = calculate_accuracy(predictions, targets, roman_tokenizer)\n    \n    return {\n        'loss': avg_loss,\n        'bleu': bleu,\n        'perplexity': perplexity,\n        'cer': cer,\n        'token_accuracy': accuracy_metrics['token_accuracy'],\n        'sequence_accuracy': accuracy_metrics['sequence_accuracy']\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:31.626699Z","iopub.execute_input":"2025-09-25T09:00:31.627378Z","iopub.status.idle":"2025-09-25T09:00:31.652110Z","shell.execute_reply.started":"2025-09-25T09:00:31.627352Z","shell.execute_reply":"2025-09-25T09:00:31.651394Z"}},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"## 9. Training Function","metadata":{}},{"cell_type":"code","source":"\ndef train_epoch(model, train_loader, optimizer, criterion, teacher_forcing_ratio=0.5, scaler=None):\n    \"\"\"Train one epoch with optional AMP\"\"\"\n    model.train()\n    total_loss = 0\n    use_amp = scaler is not None and torch.cuda.is_available()\n    \n    for batch in train_loader:\n        urdu_seq = batch['urdu'].to(device)\n        roman_seq = batch['roman'].to(device)\n        \n        decoder_target = roman_seq[:, 1:]\n        \n        optimizer.zero_grad()\n        \n        if use_amp:\n            with torch.cuda.amp.autocast():\n                outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio)\n                loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n            scaler.scale(loss).backward()\n            # Unscale before clipping\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio)\n            loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(train_loader)\n\n\n\ndef translate_text(model, text, urdu_tokenizer, roman_tokenizer):\n    \"\"\"Translate a single text\"\"\"\n    model.eval()\n    \n    tokens = urdu_tokenizer.encode(text, out_type=int)\n    input_tensor = torch.tensor([tokens], dtype=torch.long).to(device)\n    \n    with torch.no_grad():\n        output = model(input_tensor)\n        predicted_tokens = output.argmax(dim=-1).squeeze().cpu().tolist()\n        \n        # Truncate prediction at first EOS token if present (EOS id = 3)\n        try:\n            eos_index = predicted_tokens.index(3)\n            predicted_tokens = predicted_tokens[:eos_index]\n        except ValueError:\n            pass\n        \n        # Remove special tokens except EOS (PAD=0, UNK=1, BOS=2)\n        clean_tokens = [t for t in predicted_tokens if t not in [0, 1, 2]]\n        \n        translated_text = roman_tokenizer.decode(clean_tokens)\n    \n    return translated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:37.394735Z","iopub.execute_input":"2025-09-25T09:00:37.395038Z","iopub.status.idle":"2025-09-25T09:00:37.403679Z","shell.execute_reply.started":"2025-09-25T09:00:37.395014Z","shell.execute_reply":"2025-09-25T09:00:37.402947Z"}},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":"## 10. Translation Function","metadata":{}},{"cell_type":"code","source":"def translate_text(model, text, urdu_tokenizer, roman_tokenizer, max_length=50):\n    \"\"\"Improved translation function\"\"\"\n    model.eval()\n    \n    # Tokenize input\n    tokens = urdu_tokenizer.encode(text, out_type=int)\n    input_tensor = torch.tensor([tokens], dtype=torch.long).to(device)\n    \n    with torch.no_grad():\n        # Generate translation\n        output = model(input_tensor)\n        predicted_tokens = output.argmax(dim=-1).squeeze().cpu().tolist()\n        \n        # Remove special tokens\n        clean_tokens = [t for t in predicted_tokens if t not in [0, 1, 2, 3]]\n        \n        # Decode to text\n        translated_text = roman_tokenizer.decode(clean_tokens)\n        \n    return translated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:39.443511Z","iopub.execute_input":"2025-09-25T09:00:39.444061Z","iopub.status.idle":"2025-09-25T09:00:39.448906Z","shell.execute_reply.started":"2025-09-25T09:00:39.444036Z","shell.execute_reply":"2025-09-25T09:00:39.448250Z"}},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":"## 11. Experiment Runner","metadata":{}},{"cell_type":"code","source":"\ndef run_experiment(config, splits, urdu_tokenizer, roman_tokenizer):\n    \"\"\"Run a single experiment with given configuration\"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"Running experiment: {config['name']}\")\n    print(f\"Config: {config}\")\n    print(f\"{'='*50}\")\n    \n    test_results = None  # Initialize test_results to avoid UnboundLocalError\n    \n    try:\n        # Create datasets\n        train_dataset = TranslationDataset(\n            splits['train']['urdu'], splits['train']['roman'], \n            urdu_tokenizer, roman_tokenizer\n        )\n        val_dataset = TranslationDataset(\n            splits['val']['urdu'], splits['val']['roman'], \n            urdu_tokenizer, roman_tokenizer\n        )\n        test_dataset = TranslationDataset(\n            splits['test']['urdu'], splits['test']['roman'], \n            urdu_tokenizer, roman_tokenizer\n        )\n        \n        # Create data loaders\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n                                 shuffle=True, collate_fn=collate_fn)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n                               shuffle=False, collate_fn=collate_fn)\n        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], \n                                shuffle=False, collate_fn=collate_fn)\n        \n        # Initialize model\n        model = Seq2SeqModel(\n            urdu_vocab_size=urdu_tokenizer.get_piece_size(),\n            roman_vocab_size=roman_tokenizer.get_piece_size(),\n            embed_dim=config['embed_dim'],\n            hidden_dim=config['hidden_dim'],\n            dropout=config['dropout'],\n            decoder_word_dropout=config.get('decoder_word_dropout', 0.1)\n        ).to(device)\n        \n        # Initialize optimizer and criterion\n        base_lr = config['learning_rate']\n        optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.01)\n        try:\n            criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n        except TypeError:\n            # Fallback if torch version doesn't support label_smoothing\n            criterion = nn.CrossEntropyLoss(ignore_index=0)\n        \n        # AMP scaler\n        scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n        \n        # LR scheduler on validation loss\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n        warmup_epochs = config.get('warmup_epochs', 2)\n        \n        # Training loop\n        best_val_bleu = 0\n        patience = 5\n        patience_counter = 0\n        \n        train_losses = []\n        val_metrics = []\n        \n        for epoch in range(config['epochs']):\n            # Warmup learning rate\n            if epoch < warmup_epochs:\n                warmup_factor = float(epoch + 1) / float(max(1, warmup_epochs))\n                for pg in optimizer.param_groups:\n                    pg['lr'] = base_lr * warmup_factor\n            current_lr = optimizer.param_groups[0]['lr']\n            \n            # Scheduled teacher forcing: decay towards 0.1\n            base_tf = config.get('teacher_forcing_ratio', 0.5)\n            tf_ratio_epoch = max(0.1, base_tf * (1.0 - (epoch / max(1, config['epochs']))))\n            \n            train_loss = train_epoch(model, train_loader, optimizer, criterion, tf_ratio_epoch, scaler)\n            train_losses.append(train_loss)\n            \n            val_results = evaluate_model(model, val_loader, criterion, roman_tokenizer)\n            val_metrics.append(val_results)\n            \n            # Step scheduler with validation loss after warmup\n            if epoch + 1 > warmup_epochs:\n                scheduler.step(val_results['loss'])\n            \n            print(f\"Epoch {epoch+1}/{config['epochs']}\")\n            print(f\"Train Loss: {train_loss:.4f} (TF={tf_ratio_epoch:.2f}, LR={current_lr:.6f})\")\n            print(f\"Val Loss: {val_results['loss']:.4f}, BLEU: {val_results['bleu']:.4f}, \"\n                  f\"Perplexity: {val_results['perplexity']:.2f}, CER: {val_results['cer']:.4f}\")\n            print(f\"Token Accuracy: {val_results['token_accuracy']:.4f}, \"\n                  f\"Sequence Accuracy: {val_results['sequence_accuracy']:.4f}\")\n            \n            # Early stopping by BLEU\n            if val_results['bleu'] > best_val_bleu:\n                best_val_bleu = val_results['bleu']\n                patience_counter = 0\n                torch.save(model.state_dict(), f\"best_model_{config['name']}.pth\")\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n        \n        # Load best model for testing\n        try:\n            model.load_state_dict(torch.load(f\"best_model_{config['name']}.pth\"))\n            \n            # Test evaluation\n            test_results = evaluate_model(model, test_loader, criterion, roman_tokenizer)\n            \n            print(f\"\\nFinal Test Results for {config['name']}:\")\n            print(f\"Test Loss: {test_results['loss']:.4f}\")\n            print(f\"Test BLEU: {test_results['bleu']:.4f}\")\n            print(f\"Test Perplexity: {test_results['perplexity']:.2f}\")\n            print(f\"Test CER: {test_results['cer']:.4f}\")\n            print(f\"Test Token Accuracy: {test_results['token_accuracy']:.4f}\")\n            print(f\"Test Sequence Accuracy: {test_results['sequence_accuracy']:.4f}\")\n            \n            # Sample translations\n            print(f\"\\nSample Translations for {config['name']}:\")\n            sample_texts = splits['test']['urdu'][:5]\n            for i, urdu_text in enumerate(sample_texts):\n                translation = translate_text(model, urdu_text, urdu_tokenizer, roman_tokenizer)\n                actual = splits['test']['roman'][i]\n                print(f\"Urdu: {urdu_text}\")\n                print(f\"Predicted: {translation}\")\n                print(f\"Actual: {actual}\")\n                print(\"-\" * 50)\n                \n        except Exception as e:\n            print(f\"Error during model loading or testing: {e}\")\n            # Create default test results if testing fails\n            test_results = {\n                'loss': float('inf'),\n                'bleu': 0.0,\n                'perplexity': float('inf'),\n                'cer': 1.0,\n                'token_accuracy': 0.0,\n                'sequence_accuracy': 0.0\n            }\n    \n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n        # Return default values if experiment fails completely\n        return {\n            'config': config,\n            'train_losses': [],\n            'val_metrics': [],\n            'test_results': {\n                'loss': float('inf'),\n                'bleu': 0.0,\n                'perplexity': float('inf'),\n                'cer': 1.0,\n                'token_accuracy': 0.0,\n                'sequence_accuracy': 0.0\n            },\n            'best_val_bleu': 0.0\n        }\n    \n    # Ensure test_results is never None\n    if test_results is None:\n        test_results = {\n            'loss': float('inf'),\n            'bleu': 0.0,\n            'perplexity': float('inf'),\n            'cer': 1.0,\n            'token_accuracy': 0.0,\n            'sequence_accuracy': 0.0\n        }\n    \n    return {\n        'config': config,\n        'train_losses': train_losses,\n        'val_metrics': val_metrics,\n        'test_results': test_results,\n        'best_val_bleu': best_val_bleu\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:41.334076Z","iopub.execute_input":"2025-09-25T09:00:41.334400Z","iopub.status.idle":"2025-09-25T09:00:41.351867Z","shell.execute_reply.started":"2025-09-25T09:00:41.334379Z","shell.execute_reply":"2025-09-25T09:00:41.351177Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"\n\ndef main():\n    \"\"\"Main function to run all experiments\"\"\"\n    print(\"Starting Urdu to Roman Transliteration Experiments\")\n    print(\"=\" * 60)\n    \n    # Load and preprocess data\n    print(\"Loading data...\")\n    processor = UrduRomanDataProcessor(\"/kaggle/input/poet-dataset/dataset\")\n    processor.load_data()\n    processor.preprocess_data()\n    \n    # Create train/val/test splits\n    print(\"Creating data splits...\")\n    splits = processor.split_data()\n    \n    print(f\"Train samples: {len(splits['train']['urdu'])}\")\n    print(f\"Validation samples: {len(splits['val']['urdu'])}\")\n    print(f\"Test samples: {len(splits['test']['urdu'])}\")\n    \n    # Create tokenizers\n    print(\"Creating tokenizers...\")\n    urdu_tokenizer, roman_tokenizer = create_tokenizers(\n        splits['train']['urdu'] + splits['val']['urdu'], \n        splits['train']['roman'] + splits['val']['roman']\n    )\n    \n    # Interactive teacher forcing prompt\n    print(\"\\nTeacher Forcing Setup\")\n    use_tf_input = input(\"Use teacher forcing during training? (y/n, default y): \").strip().lower()\n    if use_tf_input == 'n':\n        tf_ratio = 0.0\n        print(\"Teacher forcing disabled (ratio = 0.0)\")\n    else:\n        tf_ratio_input = input(\"Enter teacher forcing ratio [0.0-1.0] (default 0.5): \").strip()\n        try:\n            tf_ratio = float(tf_ratio_input) if tf_ratio_input else 0.5\n        except ValueError:\n            print(\"Invalid input. Defaulting teacher forcing ratio to 0.5\")\n            tf_ratio = 0.5\n        tf_ratio = max(0.0, min(1.0, tf_ratio))\n        print(f\"Using teacher forcing ratio: {tf_ratio}\")\n    \n    # Define experiment configurations\n    configs = [\n        {\n            'name': 'baseline',\n            'embed_dim': 128,\n            'hidden_dim': 256,\n            'dropout': 0.1,\n            'learning_rate': 0.001,\n            'batch_size': 32,\n            'epochs': 15,\n            'teacher_forcing_ratio': tf_ratio,\n            'decoder_word_dropout': 0.1,\n            'max_length': 60,\n            'warmup_epochs': 2\n        }\n    ]\n    \n    # Run experiments\n    results = []\n    for config in configs:\n        try:\n            result = run_experiment(config, splits, urdu_tokenizer, roman_tokenizer)\n            # Only add results that have both test_results and config\n            if result and 'test_results' in result and 'config' in result:\n                results.append(result)\n            else:\n                print(f\"Warning: Experiment {config['name']} returned incomplete results\")\n        except Exception as e:\n            print(f\"Error running experiment {config['name']}: {e}\")\n            import traceback\n            traceback.print_exc()\n            continue\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT SUMMARY\")\n    print(\"=\"*80)\n    \n    if not results:\n        print(\"No experiments completed successfully.\")\n        return\n    \n    print(f\"{'Experiment':<20} {'BLEU':<8} {'CER':<8} {'Perplexity':<12} {'Token Acc':<10} {'Seq Acc':<10}\")\n    print(\"-\" * 80)\n    \n    for result in results:\n        config = result.get('config', {})\n        test_results = result.get('test_results', {})\n        \n        name = config.get('name', 'Unknown')\n        bleu = test_results.get('bleu', 0.0)\n        cer = test_results.get('cer', 1.0)\n        perplexity = test_results.get('perplexity', float('inf'))\n        token_acc = test_results.get('token_accuracy', 0.0)\n        seq_acc = test_results.get('sequence_accuracy', 0.0)\n        \n        # Handle infinite perplexity for display\n        perp_str = f\"{perplexity:.2f}\" if perplexity != float('inf') else \"inf\"\n        \n        print(f\"{name:<20} {bleu:<8.4f} {cer:<8.4f} {perp_str:<12} {token_acc:<10.4f} {seq_acc:<10.4f}\")\n    \n    # Find best model\n    if results:\n        best_result = max(results, key=lambda x: x.get('test_results', {}).get('bleu', 0))\n        best_config = best_result.get('config', {})\n        best_test = best_result.get('test_results', {})\n        \n        print(f\"\\nBest Model: {best_config.get('name', 'Unknown')}\")\n        print(f\"Best BLEU Score: {best_test.get('bleu', 0.0):.4f}\")\n        print(f\"Best CER: {best_test.get('cer', 1.0):.4f}\")\n        print(f\"Best Token Accuracy: {best_test.get('token_accuracy', 0.0):.4f}\")\n        print(f\"Best Sequence Accuracy: {best_test.get('sequence_accuracy', 0.0):.4f}\")\n    \n    print(\"\\nExperiments completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:00:46.590639Z","iopub.execute_input":"2025-09-25T09:00:46.591196Z","iopub.status.idle":"2025-09-25T09:00:46.602651Z","shell.execute_reply.started":"2025-09-25T09:00:46.591172Z","shell.execute_reply":"2025-09-25T09:00:46.601937Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:01:39.513231Z","iopub.execute_input":"2025-09-25T09:01:39.513762Z","iopub.status.idle":"2025-09-25T09:12:54.861190Z","shell.execute_reply.started":"2025-09-25T09:01:39.513738Z","shell.execute_reply":"2025-09-25T09:12:54.860325Z"}},"outputs":[{"name":"stdout","text":"Starting Urdu to Roman Transliteration Experiments\n============================================================\nLoading data...\nLoading data from urdu_ghazals_rekhta dataset...\nLoaded 21003 text pairs\nPreprocessing and filtering data...\nAfter preprocessing: 20893 pairs\nCreating data splits...\nData split - Train: 10446 (50.0%), Val: 5223 (25.0%), Test: 5224 (25.0%)\nTrain samples: 10446\nValidation samples: 5223\nTest samples: 5224\nCreating tokenizers...\nUrdu tokenizer vocabulary size: 8000\nRoman tokenizer vocabulary size: 8000\n\nTeacher Forcing Setup\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Use teacher forcing during training? (y/n, default y):  y\nEnter teacher forcing ratio [0.0-1.0] (default 0.5):  0.5\n"},{"name":"stdout","text":"Using teacher forcing ratio: 0.5\n\n==================================================\nRunning experiment: baseline\nConfig: {'name': 'baseline', 'embed_dim': 128, 'hidden_dim': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'teacher_forcing_ratio': 0.5, 'decoder_word_dropout': 0.1, 'max_length': 60, 'warmup_epochs': 2}\n==================================================\nEpoch 1/15\nTrain Loss: 7.0848 (TF=0.50, LR=0.000500)\nVal Loss: 6.2607, BLEU: 0.0000, Perplexity: 523.61, CER: 1.4051\nToken Accuracy: 0.0634, Sequence Accuracy: 0.0000\nEpoch 2/15\nTrain Loss: 5.9743 (TF=0.47, LR=0.001000)\nVal Loss: 5.6316, BLEU: 0.0003, Perplexity: 279.11, CER: 1.6045\nToken Accuracy: 0.0753, Sequence Accuracy: 0.0000\nEpoch 3/15\nTrain Loss: 5.4715 (TF=0.43, LR=0.001000)\nVal Loss: 5.2749, BLEU: 0.0016, Perplexity: 195.36, CER: 1.6855\nToken Accuracy: 0.0837, Sequence Accuracy: 0.0000\nEpoch 4/15\nTrain Loss: 5.1070 (TF=0.40, LR=0.001000)\nVal Loss: 5.0965, BLEU: 0.0032, Perplexity: 163.45, CER: 1.9962\nToken Accuracy: 0.0913, Sequence Accuracy: 0.0000\nEpoch 5/15\nTrain Loss: 4.7874 (TF=0.37, LR=0.001000)\nVal Loss: 4.9989, BLEU: 0.0052, Perplexity: 148.25, CER: 2.1160\nToken Accuracy: 0.0969, Sequence Accuracy: 0.0000\nEpoch 6/15\nTrain Loss: 4.4946 (TF=0.33, LR=0.001000)\nVal Loss: 4.8782, BLEU: 0.0069, Perplexity: 131.40, CER: 2.0403\nToken Accuracy: 0.1011, Sequence Accuracy: 0.0000\nEpoch 7/15\nTrain Loss: 4.2207 (TF=0.30, LR=0.001000)\nVal Loss: 4.8794, BLEU: 0.0079, Perplexity: 131.55, CER: 2.0520\nToken Accuracy: 0.1030, Sequence Accuracy: 0.0000\nEpoch 8/15\nTrain Loss: 3.9684 (TF=0.27, LR=0.001000)\nVal Loss: 4.9164, BLEU: 0.0079, Perplexity: 136.52, CER: 2.1483\nToken Accuracy: 0.1058, Sequence Accuracy: 0.0000\nEpoch 9/15\nTrain Loss: 3.7387 (TF=0.23, LR=0.001000)\nVal Loss: 4.8952, BLEU: 0.0106, Perplexity: 133.65, CER: 2.1329\nToken Accuracy: 0.1070, Sequence Accuracy: 0.0000\nEpoch 10/15\nTrain Loss: 3.3690 (TF=0.20, LR=0.000500)\nVal Loss: 4.8969, BLEU: 0.0120, Perplexity: 133.87, CER: 2.2769\nToken Accuracy: 0.1111, Sequence Accuracy: 0.0000\nEpoch 11/15\nTrain Loss: 3.1326 (TF=0.17, LR=0.000500)\nVal Loss: 4.9567, BLEU: 0.0123, Perplexity: 142.13, CER: 2.3223\nToken Accuracy: 0.1101, Sequence Accuracy: 0.0000\nEpoch 12/15\nTrain Loss: 2.9867 (TF=0.13, LR=0.000500)\nVal Loss: 5.0151, BLEU: 0.0124, Perplexity: 150.66, CER: 2.3424\nToken Accuracy: 0.1099, Sequence Accuracy: 0.0000\nEpoch 13/15\nTrain Loss: 2.7747 (TF=0.10, LR=0.000250)\nVal Loss: 5.0585, BLEU: 0.0137, Perplexity: 157.36, CER: 2.3894\nToken Accuracy: 0.1100, Sequence Accuracy: 0.0000\nEpoch 14/15\nTrain Loss: 2.6448 (TF=0.10, LR=0.000250)\nVal Loss: 5.1025, BLEU: 0.0140, Perplexity: 164.43, CER: 2.3670\nToken Accuracy: 0.1110, Sequence Accuracy: 0.0000\nEpoch 15/15\nTrain Loss: 2.5662 (TF=0.10, LR=0.000250)\nVal Loss: 5.1565, BLEU: 0.0137, Perplexity: 173.55, CER: 2.3964\nToken Accuracy: 0.1096, Sequence Accuracy: 0.0000\n\nFinal Test Results for baseline:\nTest Loss: 5.1945\nTest BLEU: 0.0135\nTest Perplexity: 180.28\nTest CER: 2.3998\nTest Token Accuracy: 0.1045\nTest Sequence Accuracy: 0.0000\n\nSample Translations for baseline:\nUrdu:       \nPredicted: dard-e-dil dard hai mire lam-e-dariy- firq-e-zindag hai\nActual: ai hud dard-e-dil hai bahshish-e-dost\n--------------------------------------------------\nUrdu:      \nPredicted: ab to ik z-tar de apne de apne de dekh de hu de hu de pahuche de hu\nActual: zindag ab to ik tamann de\n--------------------------------------------------\nUrdu:       \nPredicted: pa hai ke ke me me chale hu me me ke me ky me mai me abh me mai me\nActual: kais baith hai chhup ke patto me\n--------------------------------------------------\nUrdu:         \nPredicted: . kar zab kar yaad hai ye ko ko hat dekhn ye tarah s hai det hai. us ko bht bh\nActual: 'firq' aksar badal kar bhes milt hai ko kfir\n--------------------------------------------------\nUrdu:          \nPredicted: lag unhe. pe use ki me aa ga.e.e.e gay ho gay aa.e hoe hoe\nActual: haath rakh de mir kho pe ki niid aa jaa.e\n--------------------------------------------------\n\n================================================================================\nEXPERIMENT SUMMARY\n================================================================================\nExperiment           BLEU     CER      Perplexity   Token Acc  Seq Acc   \n--------------------------------------------------------------------------------\nbaseline             0.0135   2.3998   180.28       0.1045     0.0000    \n\nBest Model: baseline\nBest BLEU Score: 0.0135\nBest CER: 2.3998\nBest Token Accuracy: 0.1045\nBest Sequence Accuracy: 0.0000\n\nExperiments completed!\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:43:08.846242Z","iopub.execute_input":"2025-09-25T08:43:08.846493Z","iopub.status.idle":"2025-09-25T08:57:25.470631Z","shell.execute_reply.started":"2025-09-25T08:43:08.846476Z","shell.execute_reply":"2025-09-25T08:57:25.470027Z"}},"outputs":[{"name":"stdout","text":"Starting Urdu to Roman Transliteration Experiments\n============================================================\nLoading data...\nLoading data from urdu_ghazals_rekhta dataset...\nLoaded 21003 text pairs\nPreprocessing and filtering data...\nAfter preprocessing: 20893 pairs\nCreating data splits...\nData split - Train: 10446 (50.0%), Val: 5223 (25.0%), Test: 5224 (25.0%)\nTrain samples: 10446\nValidation samples: 5223\nTest samples: 5224\nCreating tokenizers...\n","output_type":"stream"},{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /tmp/tmpsrz2zvhh.txt\n  input_format: \n  model_prefix: urdu_tokenizer\n  model_type: BPE\n  vocab_size: 800\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 1\n  bos_id: 2\n  eos_id: 3\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:   \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /tmp/tmpsrz2zvhh.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 15669 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=535995\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=55\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 15669 sentences.\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 15669\ntrainer_interface.cc(609) LOG(INFO) Done! 9059\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18269 min_freq=1\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3422 size=20 all=1516 active=1461 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=1768 size=40 all=2039 active=1984 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=1251 size=60 all=2527 active=2472 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=789 size=80 all=3165 active=3110 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=640 size=100 all=3532 active=3477 piece=\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=639 min_freq=39\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=488 size=120 all=3941 active=1391 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=398 size=140 all=4255 active=1705 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=328 size=160 all=4576 active=2026 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=275 size=180 all=4792 active=2242 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=253 size=200 all=5072 active=2522 piece=\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=253 min_freq=36\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=233 size=220 all=5304 active=1221 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=208 size=240 all=5509 active=1426 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=260 all=5745 active=1662 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=175 size=280 all=5921 active=1838 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=161 size=300 all=6163 active=2080 piece=\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=161 min_freq=32\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=320 all=6294 active=1123 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=340 all=6422 active=1251 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=137 size=360 all=6594 active=1423 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=380 all=6818 active=1647 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=116 size=400 all=7053 active=1882 piece=\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=116 min_freq=28\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=111 size=420 all=7189 active=1132 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=105 size=440 all=7298 active=1241 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=98 size=460 all=7432 active=1375 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=94 size=480 all=7600 active=1543 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=500 all=7712 active=1655 piece=\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=90 min_freq=25\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=520 all=7863 active=1147 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=540 all=7938 active=1222 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=560 all=8084 active=1368 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=580 all=8228 active=1512 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=600 all=8324 active=1608 piece=\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=71 min_freq=22\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=620 all=8434 active=1094 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=640 all=8557 active=1217 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=660 all=8652 active=1312 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=680 all=8742 active=1402 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=700 all=8793 active=1453 piece=\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=59 min_freq=20\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=720 all=8852 active=1057 piece=\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=740 all=8905 active=1110 piece=\ntrainer_interface.cc(687) LOG(INFO) Saving model: urdu_tokenizer.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: urdu_tokenizer.vocab\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /tmp/tmpjm9asu5e.txt\n  input_format: \n  model_prefix: roman_tokenizer\n  model_type: BPE\n  vocab_size: 800\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 1\n  bos_id: 2\n  eos_id: 3\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:   \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /tmp/tmpjm9asu5e.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 15669 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=570676\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=33\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 15669 sentences.\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 15669\ntrainer_interface.cc(609) LOG(INFO) Done! 12918\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18069 min_freq=1\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3028 size=20 all=1074 active=1041 piece=r\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2105 size=40 all=1666 active=1633 piece=iy\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=1333 size=60 all=2126 active=2093 piece=as\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=866 size=80 all=2595 active=2562 piece=mai\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=651 size=100 all=3051 active=3018 piece=th\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=635 min_freq=33\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=482 size=120 all=3402 active=1322 piece=phir\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=384 size=140 all=3762 active=1682 piece=zr\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=329 size=160 all=4055 active=1975 piece=aam\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=280 size=180 all=4314 active=2234 piece=dil\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=248 size=200 all=4521 active=2441 piece=aaj\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=248 min_freq=32\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=219 size=220 all=4745 active=1224 piece=in\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=197 size=240 all=5010 active=1489 piece=liye\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=260 all=5230 active=1709 piece=rag\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=167 size=280 all=5456 active=1935 piece=zindag\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=300 all=5778 active=2257 piece=rakh\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=149 min_freq=28\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=320 all=5942 active=1156 piece=jh\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=340 all=6085 active=1299 piece=chho\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=360 all=6315 active=1529 piece=sa\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=110 size=380 all=6501 active=1715 piece=naam\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=400 all=6613 active=1827 piece=ke\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=104 min_freq=25\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=420 all=6768 active=1126 piece=de\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=440 all=6928 active=1286 piece=taraf\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=90 size=460 all=7040 active=1398 piece=lah\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=480 all=7164 active=1522 piece=alam\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=79 size=500 all=7215 active=1573 piece=ahl\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=79 min_freq=22\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=520 all=7314 active=1099 piece=haq\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=70 size=540 all=7363 active=1148 piece=mit\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=560 all=7458 active=1243 piece=taa\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=580 all=7592 active=1377 piece=raah\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=600 all=7636 active=1421 piece=aas\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=62 min_freq=19\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=620 all=7746 active=1104 piece=vaf\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=58 size=640 all=7828 active=1186 piece=bur\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=660 all=7924 active=1282 piece=jm\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=680 all=7999 active=1357 piece=lab\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=700 all=8104 active=1462 piece=fur\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=51 min_freq=17\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=720 all=8166 active=1058 piece=aag\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=740 all=8222 active=1114 piece=bhuul\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=760 all=8285 active=1177 piece=shd\ntrainer_interface.cc(687) LOG(INFO) Saving model: roman_tokenizer.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: roman_tokenizer.vocab\n","output_type":"stream"},{"name":"stdout","text":"Urdu tokenizer vocabulary size: 800\nRoman tokenizer vocabulary size: 800\n\nTeacher Forcing Setup\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Use teacher forcing during training? (y/n, default y):  y\nEnter teacher forcing ratio [0.0-1.0] (default 0.5):  0.5\n"},{"name":"stdout","text":"Using teacher forcing ratio: 0.5\n\n==================================================\nRunning experiment: baseline\nConfig: {'name': 'baseline', 'embed_dim': 128, 'hidden_dim': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'teacher_forcing_ratio': 0.5, 'decoder_word_dropout': 0.1, 'max_length': 60, 'warmup_epochs': 2}\n==================================================\nEpoch 1/15\nTrain Loss: 5.7683 (TF=0.50, LR=0.000500)\nVal Loss: 5.3314, BLEU: 0.0000, Perplexity: 206.74, CER: 1.9104\nToken Accuracy: 0.0494, Sequence Accuracy: 0.0000\nEpoch 2/15\nTrain Loss: 5.2419 (TF=0.47, LR=0.001000)\nVal Loss: 4.9997, BLEU: 0.0002, Perplexity: 148.37, CER: 2.2594\nToken Accuracy: 0.0586, Sequence Accuracy: 0.0000\nEpoch 3/15\nTrain Loss: 4.8974 (TF=0.43, LR=0.001000)\nVal Loss: 4.7344, BLEU: 0.0017, Perplexity: 113.80, CER: 2.0884\nToken Accuracy: 0.0637, Sequence Accuracy: 0.0000\nEpoch 4/15\nTrain Loss: 4.5910 (TF=0.40, LR=0.001000)\nVal Loss: 4.5471, BLEU: 0.0045, Perplexity: 94.36, CER: 2.3742\nToken Accuracy: 0.0680, Sequence Accuracy: 0.0000\nEpoch 5/15\nTrain Loss: 4.3200 (TF=0.37, LR=0.001000)\nVal Loss: 4.4175, BLEU: 0.0038, Perplexity: 82.89, CER: 2.4816\nToken Accuracy: 0.0714, Sequence Accuracy: 0.0000\nEpoch 6/15\nTrain Loss: 4.0808 (TF=0.33, LR=0.001000)\nVal Loss: 4.3244, BLEU: 0.0049, Perplexity: 75.52, CER: 2.4524\nToken Accuracy: 0.0746, Sequence Accuracy: 0.0000\nEpoch 7/15\nTrain Loss: 3.8576 (TF=0.30, LR=0.001000)\nVal Loss: 4.2817, BLEU: 0.0061, Perplexity: 72.36, CER: 2.5507\nToken Accuracy: 0.0751, Sequence Accuracy: 0.0000\nEpoch 8/15\nTrain Loss: 3.6762 (TF=0.27, LR=0.001000)\nVal Loss: 4.2368, BLEU: 0.0064, Perplexity: 69.19, CER: 2.4734\nToken Accuracy: 0.0766, Sequence Accuracy: 0.0000\nEpoch 9/15\nTrain Loss: 3.5130 (TF=0.23, LR=0.001000)\nVal Loss: 4.2180, BLEU: 0.0069, Perplexity: 67.90, CER: 2.5391\nToken Accuracy: 0.0759, Sequence Accuracy: 0.0000\nEpoch 10/15\nTrain Loss: 3.3752 (TF=0.20, LR=0.001000)\nVal Loss: 4.2447, BLEU: 0.0076, Perplexity: 69.73, CER: 2.5689\nToken Accuracy: 0.0778, Sequence Accuracy: 0.0000\nEpoch 11/15\nTrain Loss: 3.2440 (TF=0.17, LR=0.001000)\nVal Loss: 4.2194, BLEU: 0.0078, Perplexity: 67.99, CER: 2.5782\nToken Accuracy: 0.0779, Sequence Accuracy: 0.0000\nEpoch 12/15\nTrain Loss: 3.1330 (TF=0.13, LR=0.001000)\nVal Loss: 4.2379, BLEU: 0.0078, Perplexity: 69.26, CER: 2.5982\nToken Accuracy: 0.0775, Sequence Accuracy: 0.0000\nEpoch 13/15\nTrain Loss: 2.8810 (TF=0.10, LR=0.000500)\nVal Loss: 4.2481, BLEU: 0.0088, Perplexity: 69.97, CER: 2.7006\nToken Accuracy: 0.0808, Sequence Accuracy: 0.0000\nEpoch 14/15\nTrain Loss: 2.6986 (TF=0.10, LR=0.000500)\nVal Loss: 4.2660, BLEU: 0.0100, Perplexity: 71.24, CER: 2.6933\nToken Accuracy: 0.0802, Sequence Accuracy: 0.0000\nEpoch 15/15\nTrain Loss: 2.6049 (TF=0.10, LR=0.000500)\nVal Loss: 4.3186, BLEU: 0.0110, Perplexity: 75.08, CER: 2.6946\nToken Accuracy: 0.0788, Sequence Accuracy: 0.0000\n\nFinal Test Results for baseline:\nTest Loss: 4.3093\nTest BLEU: 0.0104\nTest Perplexity: 74.39\nTest CER: 2.6750\nTest Token Accuracy: 0.0789\nTest Sequence Accuracy: 0.0000\n\nSample Translations for baseline:\nUrdu:       \nPredicted: t dil-e-sh hai bah dish-e-shishazish-e-hudst haistish\nActual: ai hud dard-e-dil hai bahshish-e-dost\n--------------------------------------------------\nUrdu:      \nPredicted: ab to t ikt de nah de karte de kah de hu kah hu kah hu de ro de\nActual: zindag ab to ik tamann de\n--------------------------------------------------\nUrdu:       \nPredicted: duniy hai chh hai patno ke kh ke me mai me mai me mai me ke me ke me hai me hai me ashk me\nActual: kais baith hai chhup ke patto me\n--------------------------------------------------\nUrdu:         \nPredicted: firq' tir smb bhr kare hai ko k ko az nah hai magars hai k hai badal hai magar hai badal haieg k hai\nActual: 'firq' aksar badal kar bhes milt hai ko kfir\n--------------------------------------------------\nUrdu:          \nPredicted: lag dete mir mir shn me na aa jaa.e.e kyege jaa.egeege aa.e\nActual: haath rakh de mir kho pe ki niid aa jaa.e\n--------------------------------------------------\n\n================================================================================\nEXPERIMENT SUMMARY\n================================================================================\nExperiment           BLEU     CER      Perplexity   Token Acc  Seq Acc   \n--------------------------------------------------------------------------------\nbaseline             0.0104   2.6750   74.39        0.0789     0.0000    \n\nBest Model: baseline\nBest BLEU Score: 0.0104\nBest CER: 2.6750\nBest Token Accuracy: 0.0789\nBest Sequence Accuracy: 0.0000\n\nExperiments completed!\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XLSTM \n","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport pickle\nimport random\nimport math\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\nimport unicodedata\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Core libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\n\n# Install required packages (run in Kaggle)\n# !pip install sentencepiece\n\nimport sentencepiece as spm\n\n# Set random seeds\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:01.086201Z","iopub.execute_input":"2025-09-25T09:27:01.086984Z","iopub.status.idle":"2025-09-25T09:27:01.094409Z","shell.execute_reply.started":"2025-09-25T09:27:01.086939Z","shell.execute_reply":"2025-09-25T09:27:01.093718Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"\nclass UrduRomanDataProcessor:\n    \"\"\"Data processor for Urdu-Roman translation pairs from urdu_ghazals_rekhta dataset\"\"\"\n    \n    def __init__(self, dataset_path: str):\n        self.dataset_path = Path(dataset_path)\n        self.urdu_texts = []\n        self.roman_texts = []\n    \n    def load_data(self):\n        \"\"\"Load data from the urdu_ghazals_rekhta dataset structure\"\"\"\n        print(\"Loading data from urdu_ghazals_rekhta dataset...\")\n        \n        if not self.dataset_path.exists():\n            raise FileNotFoundError(f\"Dataset path {self.dataset_path} not found\")\n        \n        # Dataset structure: poets -> [ur, en, hi] -> files (no extensions)\n        for poet_dir in self.dataset_path.iterdir():\n            if not poet_dir.is_dir() or poet_dir.name.startswith('.'):\n                continue\n                \n            urdu_dir = poet_dir / 'ur'\n            english_dir = poet_dir / 'en'  # This contains Roman Urdu transliteration\n            \n            if not (urdu_dir.exists() and english_dir.exists()):\n                continue\n            \n            # Get all Urdu files (no extension filter needed)\n            urdu_files = [f for f in urdu_dir.iterdir() if f.is_file() and not f.name.startswith('.')]\n            \n            for urdu_file in urdu_files:\n                english_file = english_dir / urdu_file.name\n                \n                if english_file.exists() and english_file.is_file():\n                    try:\n                        # Read Urdu text\n                        with open(urdu_file, 'r', encoding='utf-8') as f:\n                            urdu_content = f.read().strip()\n                        \n                        # Read Roman Urdu text\n                        with open(english_file, 'r', encoding='utf-8') as f:\n                            roman_content = f.read().strip()\n                        \n                        # Split by lines to get verse pairs\n                        urdu_lines = [line.strip() for line in urdu_content.split('\\n') if line.strip()]\n                        roman_lines = [line.strip() for line in roman_content.split('\\n') if line.strip()]\n                        \n                        # Pair up lines (verses)\n                        for urdu_line, roman_line in zip(urdu_lines, roman_lines):\n                            if urdu_line and roman_line:\n                                self.urdu_texts.append(urdu_line)\n                                self.roman_texts.append(roman_line)\n                                \n                    except Exception as e:\n                        print(f\"Error reading {urdu_file.name}: {e}\")\n                        continue\n        \n        print(f\"Loaded {len(self.urdu_texts)} text pairs\")\n        \n        if len(self.urdu_texts) == 0:\n            raise ValueError(\"No data loaded. Check dataset structure and paths.\")\n    \n    def clean_text(self, text: str, is_urdu: bool = True) -> str:\n        \"\"\"Clean and normalize text\"\"\"\n        # Unicode normalization\n        text = unicodedata.normalize('NFKC', text)\n        \n        # Remove extra whitespace\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        if is_urdu:\n            # Keep Urdu characters and basic punctuation\n            text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\\"\\']+', '', text)\n        else:\n            # Convert to lowercase and keep Roman characters\n            text = text.lower()\n            text = re.sub(r'[^a-z0-9\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\\"\\']+', '', text)\n        \n        return text.strip()\n    \n    def preprocess_data(self, min_words=3, max_words=50):\n        \"\"\"Clean and filter the data\"\"\"\n        print(\"Preprocessing and filtering data...\")\n        \n        cleaned_urdu = []\n        cleaned_roman = []\n        \n        for urdu, roman in zip(self.urdu_texts, self.roman_texts):\n            # Clean texts\n            clean_urdu = self.clean_text(urdu, is_urdu=True)\n            clean_roman = self.clean_text(roman, is_urdu=False)\n            \n            # Filter by length\n            urdu_words = len(clean_urdu.split())\n            roman_words = len(clean_roman.split())\n            \n            if (min_words <= urdu_words <= max_words and \n                min_words <= roman_words <= max_words and \n                clean_urdu and clean_roman):\n                cleaned_urdu.append(clean_urdu)\n                cleaned_roman.append(clean_roman)\n        \n        self.urdu_texts = cleaned_urdu\n        self.roman_texts = cleaned_roman\n        \n        print(f\"After preprocessing: {len(self.urdu_texts)} pairs\")\n        \n        if len(self.urdu_texts) < 100:\n            print(\"Warning: Very few text pairs available. Consider relaxing filtering criteria.\")\n    \n    def split_data(self, test_size=0.25, val_size=0.25, random_state=42):\n        \"\"\"Split data into train/val/test sets (50/25/25 as required)\"\"\"\n        # First split: separate test set (25%)\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            self.urdu_texts, self.roman_texts, \n            test_size=test_size, random_state=random_state\n        )\n        \n        # Second split: divide remaining into train/val\n        val_adjusted = val_size / (1 - test_size)  # 0.25 / 0.75 = 0.333\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_temp, y_temp, \n            test_size=val_adjusted, random_state=random_state\n        )\n        \n        print(f\"Data split - Train: {len(X_train)} ({len(X_train)/len(self.urdu_texts)*100:.1f}%), \"\n              f\"Val: {len(X_val)} ({len(X_val)/len(self.urdu_texts)*100:.1f}%), \"\n              f\"Test: {len(X_test)} ({len(X_test)/len(self.urdu_texts)*100:.1f}%)\")\n        \n        return {\n            'train': {'urdu': X_train, 'roman': y_train},\n            'val': {'urdu': X_val, 'roman': y_val},\n            'test': {'urdu': X_test, 'roman': y_test}\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:03.422324Z","iopub.execute_input":"2025-09-25T09:27:03.422597Z","iopub.status.idle":"2025-09-25T09:27:03.436462Z","shell.execute_reply.started":"2025-09-25T09:27:03.422576Z","shell.execute_reply":"2025-09-25T09:27:03.435895Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# ===================== TOKENIZATION =====================\n\ndef create_tokenizers(train_urdu, train_roman, vocab_size=4000):\n    \n    print(f\"Training SentencePiece tokenizers with vocab_size={vocab_size}...\")\n    \n    # Create tokenizers directory\n    os.makedirs('tokenizers', exist_ok=True)\n    \n    # Save training data\n    with open('tokenizers/urdu_train.txt', 'w', encoding='utf-8') as f:\n        for text in train_urdu:\n            f.write(text + '\\n')\n    \n    with open('tokenizers/roman_train.txt', 'w', encoding='utf-8') as f:\n        for text in train_roman:\n            f.write(text + '\\n')\n    \n    # Estimate reasonable vocab size based on data\n    def estimate_vocab_size(texts, target_size):\n        all_text = ' '.join(texts)\n        unique_chars = len(set(all_text))\n        unique_words = len(set(all_text.split()))\n        # Use target size but cap at reasonable limits\n        return min(target_size, max(unique_chars * 10, 1000), unique_words)\n    \n    urdu_vocab_size = estimate_vocab_size(train_urdu, vocab_size)\n    roman_vocab_size = estimate_vocab_size(train_roman, vocab_size)\n    \n    print(f\"Adjusted vocab sizes - Urdu: {urdu_vocab_size}, Roman: {roman_vocab_size}\")\n    \n    # Train Urdu tokenizer\n    spm.SentencePieceTrainer.train(\n        input='tokenizers/urdu_train.txt',\n        model_prefix='tokenizers/urdu_tokenizer',\n        vocab_size=urdu_vocab_size,\n        model_type='unigram',\n        character_coverage=1.0,\n        pad_id=0, unk_id=1, bos_id=2, eos_id=3\n    )\n    \n    # Train Roman tokenizer\n    spm.SentencePieceTrainer.train(\n        input='tokenizers/roman_train.txt',\n        model_prefix='tokenizers/roman_tokenizer',\n        vocab_size=roman_vocab_size,\n        model_type='unigram',\n        character_coverage=1.0,\n        pad_id=0, unk_id=1, bos_id=2, eos_id=3\n    )\n    \n    # Load trained models\n    urdu_tokenizer = spm.SentencePieceProcessor(model_file='tokenizers/urdu_tokenizer.model')\n    roman_tokenizer = spm.SentencePieceProcessor(model_file='tokenizers/roman_tokenizer.model')\n    \n    print(f\"Final vocab sizes - Urdu: {urdu_tokenizer.get_piece_size()}, \"\n          f\"Roman: {roman_tokenizer.get_piece_size()}\")\n    \n    return urdu_tokenizer, roman_tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:03.935527Z","iopub.execute_input":"2025-09-25T09:27:03.935774Z","iopub.status.idle":"2025-09-25T09:27:03.943273Z","shell.execute_reply.started":"2025-09-25T09:27:03.935758Z","shell.execute_reply":"2025-09-25T09:27:03.942451Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"# ===================== DATASET AND DATALOADER =====================\n\nclass TranslationDataset(Dataset):\n    \"\"\"Dataset for translation pairs\"\"\"\n    \n    def __init__(self, urdu_texts, roman_texts, urdu_tokenizer, roman_tokenizer, max_length=50):\n        self.urdu_texts = urdu_texts\n        self.roman_texts = roman_texts\n        self.urdu_tokenizer = urdu_tokenizer\n        self.roman_tokenizer = roman_tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.urdu_texts)\n    \n    def __getitem__(self, idx):\n        urdu_text = self.urdu_texts[idx]\n        roman_text = self.roman_texts[idx]\n        \n        # Tokenize\n        urdu_tokens = self.urdu_tokenizer.encode(urdu_text, out_type=int)\n        roman_tokens = self.roman_tokenizer.encode(roman_text, out_type=int)\n        \n        # Ensure BOS and EOS in target and respect max_length\n        roman_tokens = roman_tokens[:max(0, self.max_length - 2)]\n        roman_tokens = [2] + roman_tokens + [3]\n        \n        # Truncate if necessary\n        urdu_tokens = urdu_tokens[:self.max_length]\n        \n        return {\n            'urdu': torch.tensor(urdu_tokens, dtype=torch.long),\n            'roman': torch.tensor(roman_tokens, dtype=torch.long),\n            'urdu_text': urdu_text,\n            'roman_text': roman_text\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:04.751155Z","iopub.execute_input":"2025-09-25T09:27:04.751407Z","iopub.status.idle":"2025-09-25T09:27:04.757675Z","shell.execute_reply.started":"2025-09-25T09:27:04.751390Z","shell.execute_reply":"2025-09-25T09:27:04.756912Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Collate function with padding\"\"\"\n    urdu_seqs = [item['urdu'] for item in batch]\n    roman_seqs = [item['roman'] for item in batch]\n    \n    # Pad sequences\n    urdu_padded = nn.utils.rnn.pad_sequence(urdu_seqs, batch_first=True, padding_value=0)\n    roman_padded = nn.utils.rnn.pad_sequence(roman_seqs, batch_first=True, padding_value=0)\n    \n    return {\n        'urdu': urdu_padded,\n        'roman': roman_padded,\n        'urdu_texts': [item['urdu_text'] for item in batch],\n        'roman_texts': [item['roman_text'] for item in batch]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:05.346891Z","iopub.execute_input":"2025-09-25T09:27:05.347179Z","iopub.status.idle":"2025-09-25T09:27:05.351959Z","shell.execute_reply.started":"2025-09-25T09:27:05.347157Z","shell.execute_reply":"2025-09-25T09:27:05.351381Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"# ===================== MODEL ARCHITECTURE =====================\n\nclass xLSTMEncoder(nn.Module):\n    \"\"\"xLSTM-style Encoder: BiLSTM with LayerNorm on outputs for stability\"\"\"\n    \n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.1):\n        super(xLSTMEncoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim, hidden_dim, num_layers,\n            batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.out_norm = nn.LayerNorm(hidden_dim * 2)\n        \n    def forward(self, x, lengths=None):\n        # x: (batch_size, seq_len)\n        \n        # Embedding with dropout\n        embedded = self.dropout(self.embedding(x))\n        \n        # Pack padded sequence for efficiency if lengths provided\n        if lengths is not None:\n            packed = nn.utils.rnn.pack_padded_sequence(\n                embedded, lengths, batch_first=True, enforce_sorted=False\n            )\n            outputs, (hidden, cell) = self.lstm(packed)\n            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        else:\n            outputs, (hidden, cell) = self.lstm(embedded)\n        \n        # LayerNorm over the feature dimension\n        outputs = self.out_norm(outputs)\n        \n        return outputs, hidden, cell\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:05.708208Z","iopub.execute_input":"2025-09-25T09:27:05.708698Z","iopub.status.idle":"2025-09-25T09:27:05.715266Z","shell.execute_reply.started":"2025-09-25T09:27:05.708673Z","shell.execute_reply":"2025-09-25T09:27:05.714491Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"class xLSTMDecoder(nn.Module):\n    \"\"\"xLSTM-style Decoder: LSTM with LayerNorm on hidden and output, weight tying preserved\"\"\"\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=4, dropout=0.1):\n        super(xLSTMDecoder, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.context_proj = nn.Linear(hidden_dim * 2, embed_dim)\n        self.lstm = nn.LSTM(embed_dim * 2, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.h2e = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.out_norm = nn.LayerNorm(hidden_dim)\n        self.h_norm = nn.LayerNorm(hidden_dim)\n    \n    def forward(self, x, hidden_state, encoder_outputs, encoder_mask=None):\n        if x.dim() == 1:\n            x = x.unsqueeze(1)\n        embedded = self.dropout(self.embedding(x))\n        if encoder_mask is not None:\n            lengths = encoder_mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n            context = (encoder_outputs * encoder_mask.unsqueeze(-1)).sum(dim=1) / lengths\n        else:\n            context = encoder_outputs.mean(dim=1)\n        context = self.context_proj(context).unsqueeze(1)\n        dec_input = torch.cat([embedded, context], dim=-1)\n        output, (h, c) = self.lstm(dec_input, hidden_state)\n        # LayerNorm on hidden/output\n        output = self.out_norm(output)\n        h = self.h_norm(h)\n        logits = F.linear(self.h2e(output), self.embedding.weight)\n        return logits, (h, c), None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:05.903313Z","iopub.execute_input":"2025-09-25T09:27:05.903581Z","iopub.status.idle":"2025-09-25T09:27:05.910836Z","shell.execute_reply.started":"2025-09-25T09:27:05.903560Z","shell.execute_reply":"2025-09-25T09:27:05.910072Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"class LSTMDecoder(nn.Module):\n    \"\"\"LSTM Decoder without attention (4 layers as required)\"\"\"\n    \n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=4, dropout=0.1):\n        super(LSTMDecoder, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        \n        # Fixed encoder context: project mean-pooled encoder outputs to embedding dim\n        self.context_proj = nn.Linear(hidden_dim * 2, embed_dim)\n        \n        # Decoder operates on [embedding ; fixed_context]\n        self.lstm = nn.LSTM(\n            embed_dim * 2, hidden_dim, num_layers,\n            batch_first=True, dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Weight tying: map hidden_dim -> embed_dim, then tie to embedding weight for logits\n        self.h2e = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, hidden_state, encoder_outputs, encoder_mask=None):\n        # x: (batch_size, 1) - single time step\n        if x.dim() == 1:\n            x = x.unsqueeze(1)\n        \n        embedded = self.dropout(self.embedding(x))\n        \n        # Compute fixed context (masked mean over encoder sequence)\n        if encoder_mask is not None:\n            lengths = encoder_mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n            context = (encoder_outputs * encoder_mask.unsqueeze(-1)).sum(dim=1) / lengths\n        else:\n            context = encoder_outputs.mean(dim=1)\n        context = self.context_proj(context).unsqueeze(1)  # (batch, 1, embed_dim)\n        \n        # Concatenate embedding with fixed context\n        dec_input = torch.cat([embedded, context], dim=-1)\n        \n        output, hidden_state = self.lstm(dec_input, hidden_state)\n        \n        # Project to embedding space and tie with embedding weights to produce logits\n        logits = F.linear(self.h2e(output), self.embedding.weight)\n        \n        return logits, hidden_state, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:06.094823Z","iopub.execute_input":"2025-09-25T09:27:06.095511Z","iopub.status.idle":"2025-09-25T09:27:06.102601Z","shell.execute_reply.started":"2025-09-25T09:27:06.095474Z","shell.execute_reply":"2025-09-25T09:27:06.101830Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"class Seq2SeqModel(nn.Module):\n    \"\"\"Seq2Seq model with BiLSTM encoder and LSTM decoder\"\"\"\n    \n    def __init__(self, urdu_vocab_size, roman_vocab_size, embed_dim=128, hidden_dim=256,\n                 encoder_layers=2, decoder_layers=4, dropout=0.1, decoder_word_dropout=0.0):\n        super(Seq2SeqModel, self).__init__()\n        \n        self.encoder = xLSTMEncoder(urdu_vocab_size, embed_dim, hidden_dim, encoder_layers, dropout)\n        self.decoder = xLSTMDecoder(roman_vocab_size, embed_dim, hidden_dim, decoder_layers, dropout)\n        \n        self.hidden_dim = hidden_dim\n        self.encoder_layers = encoder_layers\n        self.decoder_layers = decoder_layers\n        \n        self.bridge_h = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.bridge_c = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        # Decoder word dropout probability used during teacher forcing\n        self.decoder_word_dropout = decoder_word_dropout\n\n    def forward(self, urdu_seq, roman_seq=None, teacher_forcing_ratio=0.5):\n        batch_size = urdu_seq.size(0)\n        device = urdu_seq.device\n        \n        encoder_mask = (urdu_seq != 0).float()\n        encoder_lengths = encoder_mask.sum(dim=1).cpu()\n        \n        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(urdu_seq, encoder_lengths)\n        \n        encoder_hidden = encoder_hidden.view(self.encoder_layers, 2, batch_size, self.hidden_dim)\n        encoder_cell = encoder_cell.view(self.encoder_layers, 2, batch_size, self.hidden_dim)\n        \n        last_hidden = torch.cat([encoder_hidden[-1, 0], encoder_hidden[-1, 1]], dim=1)\n        last_cell = torch.cat([encoder_cell[-1, 0], encoder_cell[-1, 1]], dim=1)\n        \n        # Use tanh bridge for stability\n        decoder_hidden = torch.tanh(self.bridge_h(last_hidden)).unsqueeze(0).repeat(self.decoder_layers, 1, 1)\n        decoder_cell = torch.tanh(self.bridge_c(last_cell)).unsqueeze(0).repeat(self.decoder_layers, 1, 1)\n        \n        if roman_seq is not None:\n            max_length = roman_seq.size(1)\n            outputs = []\n            input_token = roman_seq[:, 0:1]\n            hidden_state = (decoder_hidden, decoder_cell)\n            \n            for t in range(max_length - 1):\n                output, hidden_state, _ = self.decoder(\n                    input_token, hidden_state, encoder_outputs, encoder_mask\n                )\n                outputs.append(output)\n                \n                # Decide next input\n                if random.random() < teacher_forcing_ratio:\n                    next_token = roman_seq[:, t+1:t+2]\n                    # Apply decoder word dropout on teacher-forced inputs\n                    if self.training and self.decoder_word_dropout > 0.0:\n                        # Replace with UNK (id=1) with probability p\n                        dropout_mask = (torch.rand_like(next_token.float()) < self.decoder_word_dropout)\n                        next_token = next_token.masked_fill(dropout_mask, 1)\n                    input_token = next_token\n                else:\n                    input_token = output.argmax(dim=-1)\n            \n            return torch.cat(outputs, dim=1)\n        else:\n            # Dynamic decode length based on input length\n            dyn_max_len = int(encoder_lengths.max().item() * 1.5) + 10\n            dyn_max_len = max(dyn_max_len, 20)\n            return self.beam_search_decode(\n                encoder_outputs, encoder_mask, decoder_hidden, decoder_cell,\n                beam_size=5, max_length=dyn_max_len\n            )\n    \n    def beam_search_decode(self, encoder_outputs, encoder_mask, decoder_hidden, \n                          decoder_cell, beam_size=5, max_length=50):\n        \"\"\"Beam search decoding to avoid repetition with length penalty\"\"\"\n        batch_size = encoder_outputs.size(0)\n        device = encoder_outputs.device\n        \n        if batch_size > 1:\n            return self.greedy_decode(encoder_outputs, encoder_mask, decoder_hidden, \n                                    decoder_cell, max_length)\n        \n        beams = [([], 0.0, (decoder_hidden, decoder_cell))]\n        completed = []\n        start_token = torch.tensor([[2]], dtype=torch.long, device=device)\n        \n        alpha = 0.6  # length penalty factor\n        unk_id = 1   # UNK token id\n        \n        for step in range(max_length):\n            candidates = []\n            \n            for seq, score, hidden_state in beams:\n                if len(seq) > 0 and seq[-1] == 3:\n                    completed.append((seq, score))\n                    continue\n                \n                input_token = start_token if len(seq) == 0 else torch.tensor([[seq[-1]]], dtype=torch.long, device=device)\n                \n                output, new_hidden, _ = self.decoder(\n                    input_token, hidden_state, encoder_outputs, encoder_mask\n                )\n                \n                log_probs = F.log_softmax(output.squeeze(1), dim=-1)\n                # Penalize UNK\n                if log_probs.size(-1) > unk_id:\n                    log_probs[0, unk_id] -= 2.0\n                \n                top_k_scores, top_k_tokens = log_probs.topk(beam_size)\n                \n                for k in range(beam_size):\n                    token = top_k_tokens[0, k].item()\n                    token_score = top_k_scores[0, k].item()\n                    \n                    if len(seq) > 0 and token == seq[-1]:\n                        token_score -= 2.0\n                    if len(seq) > 3:\n                        recent = seq[-3:]\n                        if recent.count(token) > 1:\n                            token_score -= 3.0\n                    \n                    new_seq = seq + [token]\n                    new_score = score + token_score\n                    candidates.append((new_seq, new_score, new_hidden))\n            \n            candidates.sort(key=lambda x: x[1], reverse=True)\n            beams = candidates[:beam_size]\n            \n            if len(beams) == 0:\n                break\n        \n        completed.extend(beams)\n        \n        if completed:\n            def lp(s):\n                L = max(1, len(s))\n                return L ** alpha\n            best_seq = max(completed, key=lambda x: x[1] / lp(x[0]))[0]\n        else:\n            best_seq = beams[0][0] if beams else []\n        \n        if best_seq:\n            vocab_size = self.decoder.vocab_size\n            output_probs = torch.zeros(1, len(best_seq), vocab_size, device=device)\n            for i, token in enumerate(best_seq):\n                output_probs[0, i, token] = 1.0\n        else:\n            # Emit a single EOS step to avoid zero-length outputs\n            vocab_size = self.decoder.vocab_size\n            output_probs = torch.zeros(1, 1, vocab_size, device=device)\n            output_probs[0, 0, 3] = 1.0  # EOS\n        \n        return output_probs\n    \n    def greedy_decode(self, encoder_outputs, encoder_mask, decoder_hidden, \n                     decoder_cell, max_length=50):\n        \"\"\"Greedy decoding with repetition penalty\"\"\"\n        batch_size = encoder_outputs.size(0)\n        device = encoder_outputs.device\n        vocab_size = self.decoder.vocab_size\n        \n        outputs = []\n        input_token = torch.full((batch_size, 1), 2, dtype=torch.long, device=device)  # SOS\n        hidden_state = (decoder_hidden, decoder_cell)\n        \n        # Track recent tokens for repetition penalty\n        recent_tokens = []\n        unk_id = 1\n        \n        for step in range(max_length):\n            output, hidden_state, _ = self.decoder(\n                input_token, hidden_state, encoder_outputs, encoder_mask\n            )\n            \n            # Penalize UNK\n            if output.size(-1) > unk_id:\n                output[:, :, unk_id] -= 2.0\n            \n            # Apply repetition penalty\n            if len(recent_tokens) > 0:\n                for recent_token in recent_tokens[-3:]:  # Penalize last 3 tokens\n                    if recent_token < output.size(-1):\n                        output[0, 0, recent_token] -= 5.0\n            \n            outputs.append(output)\n            \n            # Get next token\n            input_token = output.argmax(dim=-1)\n            token_id = input_token.item() if batch_size == 1 else input_token[0].item()\n            \n            recent_tokens.append(token_id)\n            \n            # Stop if EOS token\n            if token_id == 3:\n                break\n        \n        return torch.cat(outputs, dim=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:06.323602Z","iopub.execute_input":"2025-09-25T09:27:06.323869Z","iopub.status.idle":"2025-09-25T09:27:06.343864Z","shell.execute_reply.started":"2025-09-25T09:27:06.323849Z","shell.execute_reply":"2025-09-25T09:27:06.343294Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"# ===================== ENHANCED TOKENIZER CREATION =====================\n\ndef create_tokenizers(urdu_texts, roman_texts, vocab_size=8000):\n    \"\"\"Create SentencePiece tokenizers for Urdu and Roman text\"\"\"\n    import tempfile\n    import os\n    \n    # Create temporary files for training data\n    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n        urdu_temp_file = f.name\n        for text in urdu_texts:\n            f.write(text + '\\n')\n    \n    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:\n        roman_temp_file = f.name\n        for text in roman_texts:\n            f.write(text + '\\n')\n    \n    try:\n        # Train Urdu tokenizer\n        spm.SentencePieceTrainer.train(\n            input=urdu_temp_file,\n            model_prefix='urdu_tokenizer',\n            vocab_size=vocab_size,\n            character_coverage=1.0,\n            model_type='bpe',\n            pad_id=0,\n            unk_id=1,\n            bos_id=2,\n            eos_id=3\n        )\n        \n        # Train Roman tokenizer\n        spm.SentencePieceTrainer.train(\n            input=roman_temp_file,\n            model_prefix='roman_tokenizer',\n            vocab_size=vocab_size,\n            character_coverage=1.0,\n            model_type='bpe',\n            pad_id=0,\n            unk_id=1,\n            bos_id=2,\n            eos_id=3\n        )\n        \n        # Load tokenizers\n        urdu_tokenizer = spm.SentencePieceProcessor()\n        urdu_tokenizer.load('urdu_tokenizer.model')\n        \n        roman_tokenizer = spm.SentencePieceProcessor()\n        roman_tokenizer.load('roman_tokenizer.model')\n        \n        print(f\"Urdu tokenizer vocabulary size: {urdu_tokenizer.get_piece_size()}\")\n        print(f\"Roman tokenizer vocabulary size: {roman_tokenizer.get_piece_size()}\")\n        \n        return urdu_tokenizer, roman_tokenizer\n        \n    finally:\n        # Clean up temporary files\n        try:\n            os.unlink(urdu_temp_file)\n            os.unlink(roman_temp_file)\n        except:\n            pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:06.711262Z","iopub.execute_input":"2025-09-25T09:27:06.712150Z","iopub.status.idle":"2025-09-25T09:27:06.718458Z","shell.execute_reply.started":"2025-09-25T09:27:06.712126Z","shell.execute_reply":"2025-09-25T09:27:06.717869Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"\ndef calculate_perplexity(model, data_loader, criterion, device):\n    \"\"\"Calculate perplexity\"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            urdu_seq = batch['urdu'].to(device)\n            roman_seq = batch['roman'].to(device)\n            \n            decoder_target = roman_seq[:, 1:]\n            # Use full teacher forcing to compute true NLL\n            outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio=1.0)\n            \n            loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n            non_pad_tokens = (decoder_target != 0).sum().item()\n            \n            total_loss += loss.item() * non_pad_tokens\n            total_tokens += non_pad_tokens\n    \n    if total_tokens == 0:\n        return float('inf')\n    \n    avg_loss = total_loss / total_tokens\n    return math.exp(avg_loss)\n\ndef calculate_cer(predictions, targets, tokenizer):\n    \"\"\"Calculate Character Error Rate\"\"\"\n    def edit_distance_cer(s1, s2):\n        if len(s1) < len(s2):\n            return edit_distance_cer(s2, s1)\n        \n        if len(s2) == 0:\n            return len(s1)\n        \n        previous_row = range(len(s2) + 1)\n        for i, c1 in enumerate(s1):\n            current_row = [i + 1]\n            for j, c2 in enumerate(s2):\n                insertions = previous_row[j + 1] + 1\n                deletions = current_row[j] + 1\n                substitutions = previous_row[j] + (c1 != c2)\n                current_row.append(min(insertions, deletions, substitutions))\n            previous_row = current_row\n        \n        return previous_row[-1]\n    \n    total_chars = 0\n    total_errors = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Convert to lists and remove special tokens\n        if hasattr(pred, 'tolist'):\n            pred_tokens = pred.tolist()\n        else:\n            pred_tokens = list(pred)\n        \n        if hasattr(target, 'tolist'):\n            target_tokens = target.tolist()\n        else:\n            target_tokens = list(target)\n        \n        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n        \n        if len(pred_clean) > 0 and len(target_clean) > 0:\n            pred_text = tokenizer.decode(pred_clean)\n            target_text = tokenizer.decode(target_clean)\n            \n            errors = edit_distance_cer(pred_text, target_text)\n            total_errors += errors\n            total_chars += len(target_text)\n    \n    return total_errors / total_chars if total_chars > 0 else 1.0\n\ndef edit_distance(s1, s2):\n    \"\"\"Calculate edit distance (Levenshtein distance) between two strings\"\"\"\n    if len(s1) < len(s2):\n        return edit_distance(s2, s1)\n    \n    if len(s2) == 0:\n        return len(s1)\n    \n    previous_row = range(len(s2) + 1)\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1\n            deletions = current_row[j] + 1\n            substitutions = previous_row[j] + (c1 != c2)\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n    \n    return previous_row[-1]\n\ndef calculate_bleu_score(predictions, targets, tokenizer):\n    \"\"\"Calculate BLEU score properly\"\"\"\n    from collections import Counter\n    \n    def get_ngrams(tokens, n):\n        if len(tokens) < n:\n            return []\n        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n    \n    def calculate_bleu(pred_tokens, target_tokens, max_n=4):\n        if len(pred_tokens) == 0 or len(target_tokens) == 0:\n            return 0.0\n        \n        precisions = []\n        for n in range(1, min(max_n + 1, len(pred_tokens) + 1)):\n            pred_ngrams = Counter(get_ngrams(pred_tokens, n))\n            target_ngrams = Counter(get_ngrams(target_tokens, n))\n            \n            if len(pred_ngrams) == 0:\n                precisions.append(0.0)\n                continue\n            \n            matches = sum((pred_ngrams & target_ngrams).values())\n            total = sum(pred_ngrams.values())\n            precision = matches / total if total > 0 else 0.0\n            precisions.append(precision)\n        \n        # Brevity penalty\n        if len(pred_tokens) == 0:\n            bp = 0.0\n        elif len(pred_tokens) < len(target_tokens):\n            bp = math.exp(1 - len(target_tokens) / len(pred_tokens))\n        else:\n            bp = 1.0\n        \n        # Geometric mean of precisions\n        if precisions and all(p > 0 for p in precisions):\n            geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n            score = bp * geo_mean\n        else:\n            score = 0.0\n        \n        return score\n    \n    total_score = 0.0\n    count = 0\n    total_errors = 0\n    total_chars = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Handle tensors\n        if hasattr(pred, 'cpu'):\n            pred = pred.cpu()\n        if hasattr(target, 'cpu'):\n            target = target.cpu()\n        \n        # Convert to lists\n        if hasattr(pred, 'tolist'):\n            pred_tokens = pred.tolist()\n        else:\n            pred_tokens = list(pred)\n        \n        if hasattr(target, 'tolist'):\n            target_tokens = target.tolist()\n        else:\n            target_tokens = list(target)\n        \n        # Remove special tokens\n        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n        \n        if len(pred_clean) > 0 and len(target_clean) > 0:\n            # Calculate BLEU score\n            bleu = calculate_bleu(pred_clean, target_clean)\n            total_score += bleu\n            count += 1\n            \n            # Calculate CER (Character Error Rate)\n            pred_text = tokenizer.decode(pred_clean)\n            target_text = tokenizer.decode(target_clean)\n            \n            if len(pred_text) > 0 and len(target_text) > 0:\n                errors = edit_distance(pred_text, target_text)\n                total_errors += errors\n                total_chars += len(target_text)\n    \n    # Return BLEU score (not CER)\n    return total_score / count if count > 0 else 0.0\n\ndef calculate_accuracy(predictions, targets, tokenizer):\n    \"\"\"Calculate token-level and sequence-level accuracy\"\"\"\n    total_tokens = 0\n    correct_tokens = 0\n    total_sequences = 0\n    correct_sequences = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Convert to lists and remove special tokens\n        if hasattr(pred, 'tolist'):\n            pred_tokens = pred.tolist()\n        else:\n            pred_tokens = list(pred)\n        \n        if hasattr(target, 'tolist'):\n            target_tokens = target.tolist()\n        else:\n            target_tokens = list(target)\n        \n        # Remove special tokens (padding, start, end, unknown)\n        pred_clean = [t for t in pred_tokens if t not in [0, 1, 2, 3]]\n        target_clean = [t for t in target_tokens if t not in [0, 1, 2, 3]]\n        \n        if len(pred_clean) > 0 and len(target_clean) > 0:\n            # Token-level accuracy\n            min_len = min(len(pred_clean), len(target_clean))\n            max_len = max(len(pred_clean), len(target_clean))\n            \n            # Count matching tokens up to the minimum length\n            matches = sum(1 for i in range(min_len) if pred_clean[i] == target_clean[i])\n            correct_tokens += matches\n            total_tokens += max_len  # Use max length to penalize length differences\n            \n            # Sequence-level accuracy (exact match)\n            if pred_clean == target_clean:\n                correct_sequences += 1\n            total_sequences += 1\n    \n    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n    sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0.0\n    \n    return {\n        'token_accuracy': token_accuracy,\n        'sequence_accuracy': sequence_accuracy\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:09.211022Z","iopub.execute_input":"2025-09-25T09:27:09.211314Z","iopub.status.idle":"2025-09-25T09:27:09.232710Z","shell.execute_reply.started":"2025-09-25T09:27:09.211294Z","shell.execute_reply":"2025-09-25T09:27:09.232032Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, criterion, teacher_forcing_ratio=0.5, scaler=None):\n    \"\"\"Train one epoch with optional AMP\"\"\"\n    model.train()\n    total_loss = 0\n    use_amp = scaler is not None and torch.cuda.is_available()\n    \n    for batch in train_loader:\n        urdu_seq = batch['urdu'].to(device)\n        roman_seq = batch['roman'].to(device)\n        \n        decoder_target = roman_seq[:, 1:]\n        \n        optimizer.zero_grad()\n        \n        if use_amp:\n            with torch.cuda.amp.autocast():\n                outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio)\n                loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n            scaler.scale(loss).backward()\n            # Unscale before clipping\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(urdu_seq, roman_seq, teacher_forcing_ratio)\n            loss = criterion(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:09.413055Z","iopub.execute_input":"2025-09-25T09:27:09.413712Z","iopub.status.idle":"2025-09-25T09:27:09.420142Z","shell.execute_reply.started":"2025-09-25T09:27:09.413690Z","shell.execute_reply":"2025-09-25T09:27:09.419418Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"def evaluate_model(model, data_loader, criterion, roman_tokenizer):\n    \"\"\"Evaluate model\"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            urdu_seq = batch['urdu'].to(device)\n            roman_seq = batch['roman'].to(device)\n            \n            decoder_target = roman_seq[:, 1:]\n            \n            # Loss calculation with teacher forcing\n            outputs_tf = model(urdu_seq, roman_seq, teacher_forcing_ratio=1.0)\n            loss = criterion(outputs_tf.reshape(-1, outputs_tf.size(-1)), decoder_target.reshape(-1))\n            non_pad_tokens = (decoder_target != 0).sum().item()\n            \n            total_loss += loss.item() * non_pad_tokens\n            total_tokens += non_pad_tokens\n            \n            # Inference predictions using decoding (beam/greedy)\n            infer_outputs = model(urdu_seq)\n            pred_tokens = infer_outputs.argmax(dim=-1)\n            # Convert to list of tensors per batch item\n            if pred_tokens.dim() == 1:\n                pred_list = [pred_tokens]\n            else:\n                pred_list = [pred_tokens[i] for i in range(pred_tokens.size(0))]\n            for i in range(len(pred_list)):\n                predictions.append(pred_list[i].cpu())\n                targets.append(decoder_target[i].cpu())\n\n    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n    bleu = calculate_bleu_score(predictions, targets, roman_tokenizer)\n    perplexity = math.exp(avg_loss) if avg_loss != float('inf') else float('inf')\n    cer = calculate_cer(predictions, targets, roman_tokenizer)\n    accuracy_metrics = calculate_accuracy(predictions, targets, roman_tokenizer)\n    \n    return {\n        'loss': avg_loss,\n        'bleu': bleu,\n        'perplexity': perplexity,\n        'cer': cer,\n        'token_accuracy': accuracy_metrics['token_accuracy'],\n        'sequence_accuracy': accuracy_metrics['sequence_accuracy']\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:09.618513Z","iopub.execute_input":"2025-09-25T09:27:09.618766Z","iopub.status.idle":"2025-09-25T09:27:09.626495Z","shell.execute_reply.started":"2025-09-25T09:27:09.618747Z","shell.execute_reply":"2025-09-25T09:27:09.625777Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"def translate_text(model, text, urdu_tokenizer, roman_tokenizer):\n    \"\"\"Translate a single text\"\"\"\n    model.eval()\n    \n    tokens = urdu_tokenizer.encode(text, out_type=int)\n    input_tensor = torch.tensor([tokens], dtype=torch.long).to(device)\n    \n    with torch.no_grad():\n        output = model(input_tensor)\n        predicted_tokens = output.argmax(dim=-1).squeeze().cpu().tolist()\n        \n        # Truncate prediction at first EOS token if present (EOS id = 3)\n        try:\n            eos_index = predicted_tokens.index(3)\n            predicted_tokens = predicted_tokens[:eos_index]\n        except ValueError:\n            pass\n        \n        # Remove special tokens except EOS (PAD=0, UNK=1, BOS=2)\n        clean_tokens = [t for t in predicted_tokens if t not in [0, 1, 2]]\n        \n        translated_text = roman_tokenizer.decode(clean_tokens)\n    \n    return translated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:09.820534Z","iopub.execute_input":"2025-09-25T09:27:09.820811Z","iopub.status.idle":"2025-09-25T09:27:09.826362Z","shell.execute_reply.started":"2025-09-25T09:27:09.820790Z","shell.execute_reply":"2025-09-25T09:27:09.825555Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"def run_experiment(config, splits, urdu_tokenizer, roman_tokenizer):\n    \"\"\"Run a single experiment with given configuration\"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"Running experiment: {config['name']}\")\n    print(f\"Config: {config}\")\n    print(f\"{'='*50}\")\n    \n    test_results = None  # Initialize test_results to avoid UnboundLocalError\n    \n    try:\n        # Optional augmentation on training split\n        aug_ratio = config.get('augment_ratio', 0.2)\n        train_urdu_aug, train_roman_aug = augment_pairs(\n            splits['train']['urdu'], splits['train']['roman'], urdu_tokenizer, roman_tokenizer, ratio=aug_ratio\n        )\n        \n        # Create datasets\n        train_dataset = TranslationDataset(\n            train_urdu_aug, train_roman_aug,\n            urdu_tokenizer, roman_tokenizer\n        )\n        val_dataset = TranslationDataset(\n            splits['val']['urdu'], splits['val']['roman'], \n            urdu_tokenizer, roman_tokenizer\n        )\n        test_dataset = TranslationDataset(\n            splits['test']['urdu'], splits['test']['roman'], \n            urdu_tokenizer, roman_tokenizer\n        )\n        \n        # Create data loaders\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n                                 shuffle=True, collate_fn=collate_fn)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n                               shuffle=False, collate_fn=collate_fn)\n        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], \n                                shuffle=False, collate_fn=collate_fn)\n        \n        # Initialize model\n        model = Seq2SeqModel(\n            urdu_vocab_size=urdu_tokenizer.get_piece_size(),\n            roman_vocab_size=roman_tokenizer.get_piece_size(),\n            embed_dim=config['embed_dim'],\n            hidden_dim=config['hidden_dim'],\n            dropout=config['dropout'],\n            decoder_word_dropout=config.get('decoder_word_dropout', 0.1)\n        ).to(device)\n        \n        # Initialize optimizer and criterion\n        base_lr = config['learning_rate']\n        optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.01)\n        try:\n            criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n        except TypeError:\n            # Fallback if torch version doesn't support label_smoothing\n            criterion = nn.CrossEntropyLoss(ignore_index=0)\n        \n        # AMP scaler\n        scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n        \n        # LR scheduler on validation loss\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n        warmup_epochs = config.get('warmup_epochs', 2)\n        \n        # Training loop\n        best_val_bleu = 0\n        patience = 5\n        patience_counter = 0\n        \n        train_losses = []\n        val_metrics = []\n        \n        for epoch in range(config['epochs']):\n            # Warmup learning rate\n            if epoch < warmup_epochs:\n                warmup_factor = float(epoch + 1) / float(max(1, warmup_epochs))\n                for pg in optimizer.param_groups:\n                    pg['lr'] = base_lr * warmup_factor\n            current_lr = optimizer.param_groups[0]['lr']\n            \n            # Scheduled teacher forcing: decay towards 0.1\n            base_tf = config.get('teacher_forcing_ratio', 0.5)\n            tf_ratio_epoch = max(0.1, base_tf * (1.0 - (epoch / max(1, config['epochs']))))\n            \n            train_loss = train_epoch(model, train_loader, optimizer, criterion, tf_ratio_epoch, scaler)\n            train_losses.append(train_loss)\n            \n            val_results = evaluate_model(model, val_loader, criterion, roman_tokenizer)\n            val_metrics.append(val_results)\n            \n            # Step scheduler with validation loss after warmup\n            if epoch + 1 > warmup_epochs:\n                scheduler.step(val_results['loss'])\n            \n            print(f\"Epoch {epoch+1}/{config['epochs']}\")\n            print(f\"Train Loss: {train_loss:.4f} (TF={tf_ratio_epoch:.2f}, LR={current_lr:.6f})\")\n            print(f\"Val Loss: {val_results['loss']:.4f}, BLEU: {val_results['bleu']:.4f}, \"\n                  f\"Perplexity: {val_results['perplexity']:.2f}, CER: {val_results['cer']:.4f}\")\n            print(f\"Token Accuracy: {val_results['token_accuracy']:.4f}, \"\n                  f\"Sequence Accuracy: {val_results['sequence_accuracy']:.4f}\")\n            \n            # Early stopping by BLEU\n            if val_results['bleu'] > best_val_bleu:\n                best_val_bleu = val_results['bleu']\n                patience_counter = 0\n                torch.save(model.state_dict(), f\"best_model_{config['name']}.pth\")\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n        \n        # Load best model for testing\n        try:\n            model.load_state_dict(torch.load(f\"best_model_{config['name']}.pth\"))\n            \n            # Test evaluation\n            test_results = evaluate_model(model, test_loader, criterion, roman_tokenizer)\n            \n            print(f\"\\nFinal Test Results for {config['name']}:\")\n            print(f\"Test Loss: {test_results['loss']:.4f}\")\n            print(f\"Test BLEU: {test_results['bleu']:.4f}\")\n            print(f\"Test Perplexity: {test_results['perplexity']:.2f}\")\n            print(f\"Test CER: {test_results['cer']:.4f}\")\n            print(f\"Test Token Accuracy: {test_results['token_accuracy']:.4f}\")\n            print(f\"Test Sequence Accuracy: {test_results['sequence_accuracy']:.4f}\")\n            \n            # Sample translations\n            print(f\"\\nSample Translations for {config['name']}:\")\n            sample_texts = splits['test']['urdu'][:5]\n            for i, urdu_text in enumerate(sample_texts):\n                translation = translate_text(model, urdu_text, urdu_tokenizer, roman_tokenizer)\n                actual = splits['test']['roman'][i]\n                print(f\"Urdu: {urdu_text}\")\n                print(f\"Predicted: {translation}\")\n                print(f\"Actual: {actual}\")\n                print(\"-\" * 50)\n                \n        except Exception as e:\n            print(f\"Error during model loading or testing: {e}\")\n            # Create default test results if testing fails\n            test_results = {\n                'loss': float('inf'),\n                'bleu': 0.0,\n                'perplexity': float('inf'),\n                'cer': 1.0,\n                'token_accuracy': 0.0,\n                'sequence_accuracy': 0.0\n            }\n    \n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n        # Return default values if experiment fails completely\n        return {\n            'config': config,\n            'train_losses': [],\n            'val_metrics': [],\n            'test_results': {\n                'loss': float('inf'),\n                'bleu': 0.0,\n                'perplexity': float('inf'),\n                'cer': 1.0,\n                'token_accuracy': 0.0,\n                'sequence_accuracy': 0.0\n            },\n            'best_val_bleu': 0.0\n        }\n    \n    # Ensure test_results is never None\n    if test_results is None:\n        test_results = {\n            'loss': float('inf'),\n            'bleu': 0.0,\n            'perplexity': float('inf'),\n            'cer': 1.0,\n            'token_accuracy': 0.0,\n            'sequence_accuracy': 0.0\n        }\n    \n    return {\n        'config': config,\n        'train_losses': train_losses,\n        'val_metrics': val_metrics,\n        'test_results': test_results,\n        'best_val_bleu': best_val_bleu\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:27:12.802172Z","iopub.execute_input":"2025-09-25T09:27:12.802446Z","iopub.status.idle":"2025-09-25T09:27:12.818772Z","shell.execute_reply.started":"2025-09-25T09:27:12.802424Z","shell.execute_reply":"2025-09-25T09:27:12.818009Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"\nclass NoiseInjector:\n    \"\"\"Injects random noise into Roman Urdu text for augmentation\"\"\"\n    def __init__(self, swap_prob=0.05, delete_prob=0.03, insert_prob=0.03):\n        self.swap_prob = swap_prob\n        self.delete_prob = delete_prob\n        self.insert_prob = insert_prob\n        self.alphabet = list(\"abcdefghijklmnopqrstuvwxyz ',-?!\")\n    \n    def augment(self, text: str) -> str:\n        chars = list(text)\n        # Random swaps\n        i = 0\n        while i < len(chars) - 1:\n            if random.random() < self.swap_prob:\n                chars[i], chars[i+1] = chars[i+1], chars[i]\n                i += 2\n            else:\n                i += 1\n        # Random deletions\n        chars = [c for c in chars if not (random.random() < self.delete_prob and c != ' ')]\n        # Random insertions\n        i = 0\n        out = []\n        while i < len(chars):\n            out.append(chars[i])\n            if random.random() < self.insert_prob:\n                out.append(random.choice(self.alphabet))\n            i += 1\n        return ''.join(out)\n\nclass BackTransliterator:\n    \"\"\"Generate synthetic Urdu->Roman pairs by back-transliterating Roman to pseudo-Urdu and then back.\"\"\"\n    def __init__(self, urdu_tokenizer: spm.SentencePieceProcessor, roman_tokenizer: spm.SentencePieceProcessor):\n        self.urdu_tokenizer = urdu_tokenizer\n        self.roman_tokenizer = roman_tokenizer\n        # Simple mapping for common sequences (placeholder heuristic)\n        self.map = {\n            'kh': '', 'gh': '', 'sh': '', 'ch': '', 'th': '', 'ph': '',\n            'aa': '', 'ii': '', 'oo': '', 'ye': '', 'ka': '', 'ki': '', 'ka ': ' ',\n        }\n    \n    def roman_to_pseudo_urdu(self, text: str) -> str:\n        out = text\n        for k, v in self.map.items():\n            out = out.replace(k, v)\n        return out\n    \n    def back_transliterate_pair(self, urdu_text: str, roman_text: str) -> Tuple[str, str]:\n        # Noise on roman, then pseudo urdu from roman, pair with original roman\n        noisy_roman = roman_text\n        pseudo_urdu = self.roman_to_pseudo_urdu(noisy_roman)\n        return pseudo_urdu, roman_text\n\n# Hook augmentation into preprocessing pipeline\ndef augment_pairs(urdu_texts: List[str], roman_texts: List[str], urdu_tokenizer, roman_tokenizer, ratio: float = 0.2):\n    injector = NoiseInjector()\n    backtrans = BackTransliterator(urdu_tokenizer, roman_tokenizer)\n    n = len(urdu_texts)\n    k = max(1, int(n * ratio))\n    indices = random.sample(range(n), k)\n    aug_urdu = []\n    aug_roman = []\n    for i in indices:\n        u = urdu_texts[i]\n        r = roman_texts[i]\n        # two augmented variants: noise on roman and back-transliteration\n        aug_roman.append(injector.augment(r))\n        pu, rr = backtrans.back_transliterate_pair(u, r)\n        aug_urdu.append(pu)\n        aug_roman.append(rr)\n        aug_urdu.append(u)\n    return urdu_texts + aug_urdu, roman_texts + aug_roman\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:28:31.280331Z","iopub.execute_input":"2025-09-25T09:28:31.281027Z","iopub.status.idle":"2025-09-25T09:28:31.290992Z","shell.execute_reply.started":"2025-09-25T09:28:31.281003Z","shell.execute_reply":"2025-09-25T09:28:31.290211Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"\n\ndef main():\n    \"\"\"Main function to run all experiments\"\"\"\n    print(\"Starting Urdu to Roman Transliteration Experiments\")\n    print(\"=\" * 60)\n    \n    # Load and preprocess data\n    print(\"Loading data...\")\n    processor = UrduRomanDataProcessor(\"/kaggle/input/poet-dataset/dataset\")\n    processor.load_data()\n    processor.preprocess_data()\n    \n    # Create train/val/test splits\n    print(\"Creating data splits...\")\n    splits = processor.split_data()\n    \n    print(f\"Train samples: {len(splits['train']['urdu'])}\")\n    print(f\"Validation samples: {len(splits['val']['urdu'])}\")\n    print(f\"Test samples: {len(splits['test']['urdu'])}\")\n    \n    # Create tokenizers\n    print(\"Creating tokenizers...\")\n    urdu_tokenizer, roman_tokenizer = create_tokenizers(\n        splits['train']['urdu'] + splits['val']['urdu'], \n        splits['train']['roman'] + splits['val']['roman']\n    )\n    \n    # Interactive teacher forcing prompt\n    print(\"\\nTeacher Forcing Setup\")\n    use_tf_input = input(\"Use teacher forcing during training? (y/n, default y): \").strip().lower()\n    if use_tf_input == 'n':\n        tf_ratio = 0.0\n        print(\"Teacher forcing disabled (ratio = 0.0)\")\n    else:\n        tf_ratio_input = input(\"Enter teacher forcing ratio [0.0-1.0] (default 0.5): \").strip()\n        try:\n            tf_ratio = float(tf_ratio_input) if tf_ratio_input else 0.5\n        except ValueError:\n            print(\"Invalid input. Defaulting teacher forcing ratio to 0.5\")\n            tf_ratio = 0.5\n        tf_ratio = max(0.0, min(1.0, tf_ratio))\n        print(f\"Using teacher forcing ratio: {tf_ratio}\")\n    \n    # Define experiment configurations\n    configs = [\n        {\n            'name': 'baseline',\n            'embed_dim': 128,\n            'hidden_dim': 256,\n            'dropout': 0.1,\n            'learning_rate': 0.001,\n            'batch_size': 32,\n            'epochs': 15,\n            'teacher_forcing_ratio': tf_ratio,\n            'decoder_word_dropout': 0.1,\n            'max_length': 60,\n            'warmup_epochs': 2\n        }\n    ]\n    \n    # Run experiments\n    results = []\n    for config in configs:\n        try:\n            result = run_experiment(config, splits, urdu_tokenizer, roman_tokenizer)\n            # Only add results that have both test_results and config\n            if result and 'test_results' in result and 'config' in result:\n                results.append(result)\n            else:\n                print(f\"Warning: Experiment {config['name']} returned incomplete results\")\n        except Exception as e:\n            print(f\"Error running experiment {config['name']}: {e}\")\n            import traceback\n            traceback.print_exc()\n            continue\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT SUMMARY\")\n    print(\"=\"*80)\n    \n    if not results:\n        print(\"No experiments completed successfully.\")\n        return\n    \n    print(f\"{'Experiment':<20} {'BLEU':<8} {'CER':<8} {'Perplexity':<12} {'Token Acc':<10} {'Seq Acc':<10}\")\n    print(\"-\" * 80)\n    \n    for result in results:\n        config = result.get('config', {})\n        test_results = result.get('test_results', {})\n        \n        name = config.get('name', 'Unknown')\n        bleu = test_results.get('bleu', 0.0)\n        cer = test_results.get('cer', 1.0)\n        perplexity = test_results.get('perplexity', float('inf'))\n        token_acc = test_results.get('token_accuracy', 0.0)\n        seq_acc = test_results.get('sequence_accuracy', 0.0)\n        \n        # Handle infinite perplexity for display\n        perp_str = f\"{perplexity:.2f}\" if perplexity != float('inf') else \"inf\"\n        \n        print(f\"{name:<20} {bleu:<8.4f} {cer:<8.4f} {perp_str:<12} {token_acc:<10.4f} {seq_acc:<10.4f}\")\n    \n    # Find best model\n    if results:\n        best_result = max(results, key=lambda x: x.get('test_results', {}).get('bleu', 0))\n        best_config = best_result.get('config', {})\n        best_test = best_result.get('test_results', {})\n        \n        print(f\"\\nBest Model: {best_config.get('name', 'Unknown')}\")\n        print(f\"Best BLEU Score: {best_test.get('bleu', 0.0):.4f}\")\n        print(f\"Best CER: {best_test.get('cer', 1.0):.4f}\")\n        print(f\"Best Token Accuracy: {best_test.get('token_accuracy', 0.0):.4f}\")\n        print(f\"Best Sequence Accuracy: {best_test.get('sequence_accuracy', 0.0):.4f}\")\n    \n    print(\"\\nExperiments completed!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:28:33.292379Z","iopub.execute_input":"2025-09-25T09:28:33.293211Z","iopub.status.idle":"2025-09-25T09:43:30.270762Z","shell.execute_reply.started":"2025-09-25T09:28:33.293178Z","shell.execute_reply":"2025-09-25T09:43:30.270004Z"}},"outputs":[{"name":"stdout","text":"Starting Urdu to Roman Transliteration Experiments\n============================================================\nLoading data...\nLoading data from urdu_ghazals_rekhta dataset...\nLoaded 21003 text pairs\nPreprocessing and filtering data...\nAfter preprocessing: 20893 pairs\nCreating data splits...\nData split - Train: 10446 (50.0%), Val: 5223 (25.0%), Test: 5224 (25.0%)\nTrain samples: 10446\nValidation samples: 5223\nTest samples: 5224\nCreating tokenizers...\nUrdu tokenizer vocabulary size: 8000\nRoman tokenizer vocabulary size: 8000\n\nTeacher Forcing Setup\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Use teacher forcing during training? (y/n, default y):  y\nEnter teacher forcing ratio [0.0-1.0] (default 0.5):  0.5\n"},{"name":"stdout","text":"Using teacher forcing ratio: 0.5\n\n==================================================\nRunning experiment: baseline\nConfig: {'name': 'baseline', 'embed_dim': 128, 'hidden_dim': 256, 'dropout': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 15, 'teacher_forcing_ratio': 0.5, 'decoder_word_dropout': 0.1, 'max_length': 60, 'warmup_epochs': 2}\n==================================================\nEpoch 1/15\nTrain Loss: 7.2932 (TF=0.50, LR=0.000500)\nVal Loss: 6.1274, BLEU: 0.0007, Perplexity: 458.26, CER: 0.7330\nToken Accuracy: 0.1658, Sequence Accuracy: 0.0000\nEpoch 2/15\nTrain Loss: 6.0484 (TF=0.47, LR=0.001000)\nVal Loss: 5.6026, BLEU: 0.0035, Perplexity: 271.13, CER: 0.6946\nToken Accuracy: 0.1844, Sequence Accuracy: 0.0000\nEpoch 3/15\nTrain Loss: 5.5917 (TF=0.43, LR=0.001000)\nVal Loss: 5.2745, BLEU: 0.0067, Perplexity: 195.29, CER: 0.6501\nToken Accuracy: 0.2193, Sequence Accuracy: 0.0000\nEpoch 4/15\nTrain Loss: 5.2593 (TF=0.40, LR=0.001000)\nVal Loss: 5.0508, BLEU: 0.0117, Perplexity: 156.15, CER: 0.6438\nToken Accuracy: 0.2427, Sequence Accuracy: 0.0000\nEpoch 5/15\nTrain Loss: 4.9719 (TF=0.37, LR=0.001000)\nVal Loss: 4.8654, BLEU: 0.0193, Perplexity: 129.72, CER: 0.5893\nToken Accuracy: 0.2768, Sequence Accuracy: 0.0004\nEpoch 6/15\nTrain Loss: 4.7208 (TF=0.33, LR=0.001000)\nVal Loss: 4.7948, BLEU: 0.0242, Perplexity: 120.88, CER: 0.5755\nToken Accuracy: 0.2863, Sequence Accuracy: 0.0000\nEpoch 7/15\nTrain Loss: 4.4895 (TF=0.30, LR=0.001000)\nVal Loss: 4.6619, BLEU: 0.0295, Perplexity: 105.83, CER: 0.5680\nToken Accuracy: 0.3032, Sequence Accuracy: 0.0006\nEpoch 8/15\nTrain Loss: 4.2964 (TF=0.27, LR=0.001000)\nVal Loss: 4.6697, BLEU: 0.0373, Perplexity: 106.67, CER: 0.5502\nToken Accuracy: 0.3000, Sequence Accuracy: 0.0006\nEpoch 9/15\nTrain Loss: 4.1220 (TF=0.23, LR=0.001000)\nVal Loss: 4.5920, BLEU: 0.0418, Perplexity: 98.70, CER: 0.5391\nToken Accuracy: 0.3170, Sequence Accuracy: 0.0011\nEpoch 10/15\nTrain Loss: 3.9613 (TF=0.20, LR=0.001000)\nVal Loss: 4.5467, BLEU: 0.0416, Perplexity: 94.32, CER: 0.5468\nToken Accuracy: 0.3230, Sequence Accuracy: 0.0017\nEpoch 11/15\nTrain Loss: 3.8194 (TF=0.17, LR=0.001000)\nVal Loss: 4.5508, BLEU: 0.0459, Perplexity: 94.71, CER: 0.5314\nToken Accuracy: 0.3231, Sequence Accuracy: 0.0010\nEpoch 12/15\nTrain Loss: 3.6902 (TF=0.13, LR=0.001000)\nVal Loss: 4.5410, BLEU: 0.0519, Perplexity: 93.78, CER: 0.5279\nToken Accuracy: 0.3321, Sequence Accuracy: 0.0021\nEpoch 13/15\nTrain Loss: 3.5516 (TF=0.10, LR=0.001000)\nVal Loss: 4.5407, BLEU: 0.0542, Perplexity: 93.76, CER: 0.5175\nToken Accuracy: 0.3376, Sequence Accuracy: 0.0021\nEpoch 14/15\nTrain Loss: 3.4275 (TF=0.10, LR=0.001000)\nVal Loss: 4.5224, BLEU: 0.0553, Perplexity: 92.06, CER: 0.5173\nToken Accuracy: 0.3387, Sequence Accuracy: 0.0031\nEpoch 15/15\nTrain Loss: 3.3029 (TF=0.10, LR=0.001000)\nVal Loss: 4.5359, BLEU: 0.0595, Perplexity: 93.31, CER: 0.5112\nToken Accuracy: 0.3420, Sequence Accuracy: 0.0040\n\nFinal Test Results for baseline:\nTest Loss: 4.6343\nTest BLEU: 0.0572\nTest Perplexity: 102.95\nTest CER: 0.5205\nTest Token Accuracy: 0.3230\nTest Sequence Accuracy: 0.0033\n\nSample Translations for baseline:\nUrdu:       \nPredicted: ai hud dil-e-dil hai hud-e-ayym\nActual: ai hud dard-e-dil hai bahshish-e-dost\n--------------------------------------------------\nUrdu:      \nPredicted: zindag ab to ik jne de\nActual: zindag ab to ik tamann de\n--------------------------------------------------\nUrdu:       \nPredicted: ik liy hai huqq ke ke raste me\nActual: kais baith hai chhup ke patto me\n--------------------------------------------------\nUrdu:         \nPredicted: 'dh' tarah kar kar gay hai ko kfir\nActual: 'firq' aksar badal kar bhes milt hai ko kfir\n--------------------------------------------------\nUrdu:          \nPredicted: haath nah baith ke pe ki pe aa jaa.e\nActual: haath rakh de mir kho pe ki niid aa jaa.e\n--------------------------------------------------\n\n================================================================================\nEXPERIMENT SUMMARY\n================================================================================\nExperiment           BLEU     CER      Perplexity   Token Acc  Seq Acc   \n--------------------------------------------------------------------------------\nbaseline             0.0572   0.5205   102.95       0.3230     0.0033    \n\nBest Model: baseline\nBest BLEU Score: 0.0572\nBest CER: 0.5205\nBest Token Accuracy: 0.3230\nBest Sequence Accuracy: 0.0033\n\nExperiments completed!\n","output_type":"stream"}],"execution_count":98}]}